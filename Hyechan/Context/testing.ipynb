{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"TurkuNLP/wikibert-base-es-cased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"TurkuNLP/wikibert-base-es-cased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareInputs(init_text):\n",
    "    # List of punctuation to determine where segments end\n",
    "    punc_list = [\".\", \"?\", \"!\"]\n",
    "    # Prepend the [CLS] tag\n",
    "    prompt_text = \"[CLS] \" + init_text\n",
    "    # Insert the [SEP] tags\n",
    "    for i in range(0, len(prompt_text)):\n",
    "        if prompt_text[i] in punc_list:\n",
    "            prompt_text = prompt_text[:i + 1] + \" [SEP]\" + prompt_text[i + 1:]\n",
    "\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSegIDs(tokenized_text):\n",
    "    currentSeg = 0\n",
    "    seg_ids = []\n",
    "    for token in tokenized_text:\n",
    "        seg_ids.append(currentSeg)\n",
    "        if token == \"[SEP]\":\n",
    "            currentSeg += 1\n",
    "\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLogProb(masked_text, original_text, segment_ids, index):  \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(masked_text)\n",
    "    print(masked_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segment_tensor = torch.tensor([segment_ids])\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, token_type_ids=segment_tensor)\n",
    "        next_token_logits = outputs[0][0, index, :]\n",
    "\n",
    "    next_token_logprobs = next_token_logits - next_token_logits.logsumexp(0)\n",
    "    print(next_token_logprobs)\n",
    "\n",
    "    return next_token_logprobs[tokenizer.convert_tokens_to_ids(original_text[index])].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigContext(tokenized_text, segment_ids, index):\n",
    "    text = tokenized_text.copy()\n",
    "    text[index] = \"[MASK]\"\n",
    "    return computeLogProb(text, tokenized_text, segment_ids, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallContext(tokenized_text, segment_ids, index):\n",
    "    text = tokenized_text.copy()\n",
    "    for i in range(1, len(text) - 1):\n",
    "        if i != index - 1 and i != index + 1:\n",
    "            text[i] = \"[MASK]\"\n",
    "    return computeLogProb(text, tokenized_text, segment_ids, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[CLS] Es un día hermoso. [SEP]\n"
    }
   ],
   "source": [
    "input_text = \"Es un día hermoso.\"\n",
    "prepped_text = prepareInputs(input_text)\n",
    "print(prepped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', 'es', 'un', 'dia', 'hermos', '##o', '.', '[SEP]']\n"
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(prepped_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 0, 0, 0, 0, 0, 0]\n"
    }
   ],
   "source": [
    "segment_ids = createSegIDs(tokenized_text)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', '[MASK]', 'un', '[MASK]', 'hermos', '[MASK]', '[MASK]', '[SEP]']\ntensor([-14.8545, -14.9305, -16.4350,  ..., -17.9626, -15.8582, -18.0102])\n-8.252510070800781\n['[CLS]', 'es', 'un', '[MASK]', 'hermos', '##o', '.', '[SEP]']\ntensor([-16.3195, -17.2106, -19.6498,  ..., -19.7954, -19.6179, -18.5400])\n-9.8970365524292\n"
    }
   ],
   "source": [
    "print(smallContext(tokenized_text, segment_ids, 3))\n",
    "print(bigContext(tokenized_text, segment_ids, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segment_tensor = torch.tensor([segment_ids])\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segment_tensor)\n",
    "    next_token_logits = outputs[0][0, 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logprobs = next_token_logits - next_token_logits.logsumexp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}