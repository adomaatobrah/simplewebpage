{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: 100%|██████████| 548M/548M [00:40<00:00, 13.7MB/s]\n"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[   64,  9379,  1276, 22389,   262,  6266,  1813]])"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "prompt_text = \"a robot must obey the orders given\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_scores, past = model(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(284)"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "next_token_logits = prediction_scores[0, -1, :]\n",
    "next_token_logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[' to', ' by', ' it', ' him', ' them', ' in', '.', ',', ' the', ' and']"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "[tok.replace('Ġ', ' ') for tok in tokenizer.convert_ids_to_tokens(next_token_logits.topk(10).indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.5773)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "# return normalized probability for the 0th dimension of next_token_logits\n",
    "torch.softmax(next_token_logits, 0)[284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================!\n",
    "# Code from here on is for BERT multilingual\n",
    "# Taken from my BERT practice file\n",
    "#============================================!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_text = \"[CLS] es un dia hermoso. [SEP]\"\n",
    "token_masked_text = \"[CLS] es un [MASK] hermoso. [SEP]\"\n",
    "full_masked_text = \"[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(token_masked_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensor = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensor)\n",
    "    predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['árbol', 'bien', 'flor', 'color', 'material', 'tall', 'compuesto', 'mineral', 'dato', '.', 'ser', 'fruto', 'Sol', 'órgano', 'suelo', 'pez', 'sistema', 'solo', 'buen', ':', 'elemento', 'género', 'un', 'pequeño', 'valor', 'herb', 'sol', ',', 'agua', 'sin', 'poco', 'sal', 'ar', 'es', 'cuerpo', 'pie', 'sabor', 'simple', '-', 'porte', 'tubo', '##ce', 'polen', 'carácter', 'bin', 'crecimiento', 'h', 'tamaño', 'canto', 'animal', 'timbre', 'alto', 'alimento', 'vigor', 'ácido', 'flores', 'niño', 'vas', 'lo', 'clima', 'árboles', 'tronco', 'componente', 'sencillo', 'todo', 'terreno', 'sello', 'muy', 'campo', 'principio', 'conjunto', 'cultivo', 'aceite', '##bust', 'municipio', 'origen', '##o', 'tipo', 'Bien', 'uso', 'al', 'vino', 'líquido', 'fuego', 'producto', 'este', 'escudo', 'del', 'no', 'tax', 'vuelo', 'viento', 'planta', 'tal', '##te', 'ave', 'frutos', 'verde', 'coche', 'oso']\n"
    }
   ],
   "source": [
    "predicted_outputs = [tokenizer.convert_ids_to_tokens([index.item()])[0] for index in predictions[0, 3].topk(100).indices]\n",
    "print(predicted_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[  9580, 118762,  12092,   9663,  62200,   9248,  41919,  37388,  48549,\n            136]])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "prompt_text = \"오늘도 저랑 만날래요?\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores, past = model.forward(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text_to_translate = \"[CLS] I have a banana in Spanish: [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
    "# for i in range(0, 10):\n",
    "#     print(text_to_translate)\n",
    "#     tokenized_text = tokenizer.tokenize(text_to_translate)\n",
    "#     print(tokenized_text)\n",
    "\n",
    "#     indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#     tokens_tensor = torch.tensor([indexed_tokens])\n",
    "#     segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "#     predicted_outputs = [tokenizer.convert_ids_to_tokens([index.item()])[0] for index in predictions[0, -1].topk(10).indices]\n",
    "#     print(predicted_outputs)\n",
    "\n",
    "#     text_to_translate = text_to_translate.replace(\"[MASK]\", predicted_outputs[0])\n",
    "#     text_to_translate += \" [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def prepareInputs(init_text):\n",
    "    # List of punctuation to determine where segments end\n",
    "    punc_list = [\".\", \"?\", \"!\"]\n",
    "    # Prepend the [CLS] tag\n",
    "    prompt_text = \"[CLS] \" + init_text\n",
    "    # Insert the [SEP] tags\n",
    "    for i in range(0, len(prompt_text)):\n",
    "        if prompt_text[i] in punc_list:\n",
    "            prompt_text = prompt_text[:i + 1] + \" [SEP]\" + prompt_text[i + 1:]\n",
    "\n",
    "    return prompt_text\n",
    "\n",
    "def createSegIDs(tokenized_text):\n",
    "    currentSeg = 0\n",
    "    seg_ids = []\n",
    "    for token in tokenized_text:\n",
    "        seg_ids.append(currentSeg)\n",
    "        if token == \"[SEP]\":\n",
    "            currentSeg += 1\n",
    "\n",
    "    return seg_ids\n",
    "\n",
    "def addMask(tokenized_text, mask_word):\n",
    "    print(tokenized_text)\n",
    "    mask_indices = []\n",
    "    for i in range(0, len(tokenized_text)):\n",
    "        if tokenized_text[i] == mask_word:\n",
    "            tokenized_text[i] = \"[MASK]\"\n",
    "            mask_indices.append(i)\n",
    "\n",
    "    return (tokenized_text, mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', 'es', 'un', 'árbol', 'her', '##mos', '##o', '.', '[SEP]']\n[CLS] es un árbol hermoso. [SEP]\n['[CLS]', 'es', 'un', '[MASK]', 'her', '##mos', '##o', '.', '[SEP]']\n55220\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-0167b6c884d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mprediction_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msegment_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "text = \"es un árbol hermoso.\"\n",
    "prepped_text = prepareInputs(text)\n",
    "tokenized_text = tokenizer.tokenize(prepped_text)\n",
    "segment_ids = createSegIDs(tokenized_text)\n",
    "word_to_mask = \"árbol\"\n",
    "masked_text, mask_indices = addMask(tokenized_text, word_to_mask)\n",
    "\n",
    "print(prepped_text)\n",
    "print(masked_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(masked_text)\n",
    "masked_token_id = tokenizer.convert_tokens_to_ids(word_to_mask)\n",
    "print(masked_token_id)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segment_tensor = torch.tensor([segment_ids])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segment_tensor)\n",
    "    prediction_scores = outputs[0]\n",
    "\n",
    "for i in mask_indices:\n",
    "    predicted_outputs = [tokenizer.convert_ids_to_tokens([index.item()])[0] for index in prediction_scores[0, i].topk(10).indices]\n",
    "\n",
    "predicted_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.4303)"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "next_token_logits = prediction_scores[0, mask_indices[0], :]\n",
    "next_token_logits\n",
    "\n",
    "torch.softmax(next_token_logits, 0)[masked_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordfreq'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-f05b1efe3ce5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwordfreq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordfreq'"
     ]
    }
   ],
   "source": [
    "import wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}