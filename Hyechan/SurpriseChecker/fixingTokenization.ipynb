{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareInputs(init_text):\n",
    "    # List of punctuation to determine where segments end\n",
    "    punc_list = [\".\", \"?\", \"!\"]\n",
    "    # Prepend the [CLS] tag\n",
    "    prompt_text = \"[CLS] \" + init_text\n",
    "    # Insert the [SEP] tags\n",
    "    for i in range(0, len(prompt_text)):\n",
    "        if prompt_text[i] in punc_list:\n",
    "            prompt_text = prompt_text[:i + 1] + \" [SEP]\" + prompt_text[i + 1:]\n",
    "\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSegIDs(tokenized_text):\n",
    "    currentSeg = 0\n",
    "    seg_ids = []\n",
    "    for token in tokenized_text:\n",
    "        seg_ids.append(currentSeg)\n",
    "        if token == \"[SEP]\":\n",
    "            currentSeg += 1\n",
    "\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMask(tokenized_text, mask_word):\n",
    "    result_text = tokenized_text\n",
    "    mask_word_tokens = tokenizer.tokenize(mask_word)\n",
    "    mask_indices = []\n",
    "    for i in range(0, len(result_text)):\n",
    "        if result_text[i] in mask_word_tokens:\n",
    "            result_text[i] = \"[MASK]\"\n",
    "            mask_indices.append(i)\n",
    "\n",
    "    return (result_text, mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSinglePred(indexed_tokens, indexed_masked_tokens, segment_ids, mask_index):\n",
    "    tokens_tensor = torch.tensor([indexed_masked_tokens])\n",
    "    segment_tensor = torch.tensor([segment_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, token_type_ids=segment_tensor)\n",
    "        prediction_scores = outputs[0]\n",
    "\n",
    "    next_token_logits = prediction_scores[0, mask_index, :]\n",
    "    preds = ([tokenizer.convert_ids_to_tokens(index.item()) for index in next_token_logits.topk(5).indices])\n",
    "    prob = torch.softmax(next_token_logits, 0)[indexed_tokens[mask_index]].item()\n",
    "\n",
    "    return (preds, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getPreds(indexed_tokens,\n",
    "             indexed_masked_tokens,\n",
    "             masked_text,\n",
    "             segment_ids,\n",
    "             mask_indices,\n",
    "             totalPreds,\n",
    "             totalProbs,\n",
    "             nextSentences,\n",
    "             index):\n",
    "    preds, prob = getSinglePred(indexed_tokens, indexed_masked_tokens, segment_ids, mask_indices[index])\n",
    "    totalPreds.append(preds)\n",
    "    totalProbs.append(prob)\n",
    "\n",
    "    for next_word in preds:\n",
    "        masked_text[mask_indices[index]] = next_word\n",
    "        indexed_masked_tokens = tokenizer.convert_tokens_to_ids(masked_text)\n",
    "        if (index == len(mask_indices) - 1):\n",
    "            result = [indexed_masked_tokens[i] for i in mask_indices]\n",
    "            nextSentences.append(tokenizer.decode(result))\n",
    "        else:\n",
    "            getPreds(indexed_tokens,\n",
    "                        indexed_masked_tokens,\n",
    "                        masked_text,\n",
    "                        segment_ids,\n",
    "                        mask_indices,\n",
    "                        totalPreds,\n",
    "                        totalProbs,\n",
    "                        nextSentences,\n",
    "                        index + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[CLS] [MASK] [MASK] día [MASK]. [SEP]\n"
    }
   ],
   "source": [
    "input_text = \"[MASK] [MASK] día [MASK].\"\n",
    "word_to_mask = \"día\"\n",
    "prepped_text = prepareInputs(input_text)\n",
    "print(prepped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', '[MASK]', '[MASK]', 'día', '[MASK]', '.', '[SEP]']\n"
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(prepped_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 0, 0, 0, 0, 0]\n"
    }
   ],
   "source": [
    "segment_ids = createSegIDs(tokenized_text)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[101, 103, 103, 14184, 103, 119, 102]\n"
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']\n[3]\n"
    }
   ],
   "source": [
    "masked_text, mask_indices = addMask(tokenized_text, word_to_mask)\n",
    "print(masked_text)\n",
    "print(mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[101, 103, 103, 103, 103, 119, 102]\n"
    }
   ],
   "source": [
    "indexed_masked_tokens = tokenizer.convert_tokens_to_ids(masked_text)\n",
    "print(indexed_masked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "totalPreds = []\n",
    "totalProbs = []\n",
    "nextSentences = []\n",
    "getPreds(indexed_tokens, indexed_masked_tokens, masked_text, segment_ids, mask_indices, totalPreds, totalProbs, nextSentences, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['.', '।', ',', ':', '。']]"
     },
     "metadata": {},
     "execution_count": 289
    }
   ],
   "source": [
    "totalPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[4.460775926418137e-06]"
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "source": [
    "totalProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['.', '।', ',', ':', '。']"
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "source": [
    "nextSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Es un dia hermoso.\"\n",
    "\"Es un dia [MASK] [MASK] [MASK].\"\n",
    "\"[MASK] [MASK] dia [MASK].\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}