{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareInputs(init_text):\n",
    "    # List of punctuation to determine where segments end\n",
    "    punc_list = [\".\", \"?\", \"!\"]\n",
    "    # Prepend the [CLS] tag\n",
    "    prompt_text = \"[CLS] \" + init_text\n",
    "    # Insert the [SEP] tags\n",
    "    for i in range(0, len(prompt_text)):\n",
    "        if prompt_text[i] in punc_list:\n",
    "            prompt_text = prompt_text[:i + 1] + \" [SEP]\" + prompt_text[i + 1:]\n",
    "\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSegIDs(tokenized_text):\n",
    "    currentSeg = 0\n",
    "    seg_ids = []\n",
    "    for token in tokenized_text:\n",
    "        seg_ids.append(currentSeg)\n",
    "        if token == \"[SEP]\":\n",
    "            currentSeg += 1\n",
    "\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMask(tokenized_text, mask_word):\n",
    "    result_text = tokenized_text\n",
    "    mask_word_tokens = tokenizer.tokenize(mask_word)\n",
    "    mask_indices = []\n",
    "    for i in range(0, len(result_text)):\n",
    "        if result_text[i] in mask_word_tokens:\n",
    "            result_text[i] = \"[MASK]\"\n",
    "            mask_indices.append(i)\n",
    "\n",
    "    return (result_text, mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSinglePred(indexed_tokens, indexed_masked_tokens, segment_ids, mask_index):\n",
    "    tokens_tensor = torch.tensor([indexed_masked_tokens])\n",
    "    segment_tensor = torch.tensor([segment_ids])\n",
    "\n",
    "    probs = []\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, token_type_ids=segment_tensor)\n",
    "        prediction_scores = outputs[0]\n",
    "\n",
    "    next_token_logits = prediction_scores[0, mask_index, :]\n",
    "    preds = ([tokenizer.convert_ids_to_tokens(index.item()) for index in next_token_logits.topk(5).indices])\n",
    "    prob = torch.softmax(next_token_logits, 0)[indexed_tokens[mask_index]].item()\n",
    "\n",
    "    return (preds, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getPreds(indexed_tokens,\n",
    "             indexed_masked_tokens,\n",
    "             masked_text,\n",
    "             segment_ids,\n",
    "             mask_indices,\n",
    "             totalPreds,\n",
    "             totalProbs,\n",
    "             nextSentences,\n",
    "             index):\n",
    "    preds, prob = getSinglePred(indexed_tokens, indexed_masked_tokens, segment_ids, mask_indices[index])\n",
    "    totalPreds.append(preds)\n",
    "    totalProbs.append(prob)\n",
    "\n",
    "    for next_word in preds:\n",
    "        masked_text[mask_indices[index]] = next_word\n",
    "        indexed_masked_tokens = tokenizer.convert_tokens_to_ids(masked_text)\n",
    "        if (index == len(mask_indices) - 1):\n",
    "            nextSentences.append(tokenizer.decode(indexed_masked_tokens))\n",
    "        else:\n",
    "            getPreds(indexed_tokens,\n",
    "                        indexed_masked_tokens,\n",
    "                        masked_text,\n",
    "                        segment_ids,\n",
    "                        mask_indices,\n",
    "                        totalPreds,\n",
    "                        totalProbs,\n",
    "                        nextSentences,\n",
    "                        index + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[CLS] Es un día hermoso. [SEP]\n"
    }
   ],
   "source": [
    "input_text = \"Es un día hermoso.\"\n",
    "word_to_mask = \"hermoso\"\n",
    "prepped_text = prepareInputs(input_text)\n",
    "print(prepped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', 'Es', 'un', 'día', 'her', '##mos', '##o', '.', '[SEP]']\n"
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(prepped_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
    }
   ],
   "source": [
    "segment_ids = createSegIDs(tokenized_text)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[101, 10912, 10119, 14184, 10485, 13386, 10133, 119, 102]\n"
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', 'Es', 'un', 'día', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']\n[4, 5, 6]\n"
    }
   ],
   "source": [
    "masked_text, mask_indices = addMask(tokenized_text, word_to_mask)\n",
    "print(masked_text)\n",
    "print(mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[101, 10912, 10119, 14184, 103, 103, 103, 119, 102]\n"
    }
   ],
   "source": [
    "indexed_masked_tokens = tokenizer.convert_tokens_to_ids(masked_text)\n",
    "print(indexed_masked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "totalPreds = []\n",
    "totalProbs = []\n",
    "nextSentences = []\n",
    "getPreds(indexed_tokens, indexed_masked_tokens, masked_text, segment_ids, mask_indices, totalPreds, totalProbs, nextSentences, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['de', 'en', 'para', '.', 'a'],\n ['la', 'lu', 'las', 'los', 'lo'],\n ['vida', 'cultura', 'religión', 'educación', 'guerra'],\n ['guerra', 'guerre', '##te', '##ta', '##r'],\n ['##r', '##s', '##rs', '##t', '##ra'],\n ['##ra', '##ras', '##ro', '##ros', '##a'],\n ['##a', '##o', '##as', '##ar', '##an'],\n ['lo', 'Lo', 'li', 'po', 'll'],\n ['##an', '##án', '##ano', '##ar', '##a'],\n ['##a', '##o', '##e', '##as', '##an'],\n ['##an', '##án', '##ano', '##ar', '##a'],\n ['##a', '##o', '##as', '##u', '##ar'],\n ['##ar', '##r', '##al', '##ara', '##are'],\n ['ll', 'pal', 'l', 'lo', 'call'],\n ['##are', '##ar', '##ate', '##ari', '##aar'],\n ['##aar', '##lo', '##oo', '##ar', '##la'],\n ['##la', '##lo', '##le', '##l', '##los'],\n ['##los', '##lo', '##os', '##les', '##las'],\n ['##las', '##los', '##as', '##la', '##les'],\n ['call', 'pal', 'sal', 'bol', 'rosa'],\n ['##les', '##los', '##las', '##le', '##os'],\n ['##os', '##o', '##los', '##s', '##mos'],\n ['##mos', '##os', '##mo', 'sal', '##món'],\n ['##món', '##ón', '##lón', '##ín', '##tón'],\n ['##tón', '##ton', '##to', '##ón', '.'],\n ['rosa', 'Rosa', 'rose', 'la', '.'],\n ['.', ',', 'y', '...', ':'],\n [':', '.', '...', ',', 'y'],\n ['y', '##s', 'e', '.', 'a'],\n ['a', 'd', 'de', 'an', 'o'],\n ['o', 'a', 'e', 's', 'u']]"
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "totalPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[7.71815211919602e-06,\n 1.567037725180853e-05,\n 0.0001654300285736099,\n 0.0008031927864067256,\n 9.124159987550229e-05,\n 1.581715878273826e-05,\n 0.0041012815199792385,\n 8.052931932400753e-11,\n 3.5911118175135925e-05,\n 0.008041121065616608,\n 8.465244718536269e-06,\n 0.005998819135129452,\n 2.528444120741824e-08,\n 2.6212879089548835e-11,\n 8.283332135761157e-06,\n 0.000747158657759428,\n 2.174424116674345e-05,\n 1.202324256155407e-06,\n 4.314163959406869e-08,\n 1.2995370752832969e-06,\n 0.00010645577276591212,\n 0.0007507798727601767,\n 0.00010410712275188416,\n 0.0003608488186728209,\n 2.19695721170865e-05,\n 4.12734202370757e-09,\n 0.0003392874787095934,\n 1.2097406397515442e-05,\n 0.003711400553584099,\n 8.368590897589456e-06,\n 9.201301872963086e-05]"
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "totalProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['[CLS] Es un día de la vida. [SEP]',\n '[CLS] Es un día de la cultura. [SEP]',\n '[CLS] Es un día de la religión. [SEP]',\n '[CLS] Es un día de la educación. [SEP]',\n '[CLS] Es un día de la guerra. [SEP]',\n '[CLS] Es un día de lu guerra. [SEP]',\n '[CLS] Es un día de lu guerre. [SEP]',\n '[CLS] Es un día de lute. [SEP]',\n '[CLS] Es un día de luta. [SEP]',\n '[CLS] Es un día de lur. [SEP]',\n '[CLS] Es un día de lasr. [SEP]',\n '[CLS] Es un día de lass. [SEP]',\n '[CLS] Es un día de lasrs. [SEP]',\n '[CLS] Es un día de last. [SEP]',\n '[CLS] Es un día de lasra. [SEP]',\n '[CLS] Es un día de losra. [SEP]',\n '[CLS] Es un día de losras. [SEP]',\n '[CLS] Es un día de losro. [SEP]',\n '[CLS] Es un día de losros. [SEP]',\n '[CLS] Es un día de losa. [SEP]',\n '[CLS] Es un día de loa. [SEP]',\n '[CLS] Es un día de loo. [SEP]',\n '[CLS] Es un día de loas. [SEP]',\n '[CLS] Es un día de loar. [SEP]',\n '[CLS] Es un día de loan. [SEP]',\n '[CLS] Es un día en loan. [SEP]',\n '[CLS] Es un día en loán. [SEP]',\n '[CLS] Es un día en loano. [SEP]',\n '[CLS] Es un día en loar. [SEP]',\n '[CLS] Es un día en loa. [SEP]',\n '[CLS] Es un día en Loa. [SEP]',\n '[CLS] Es un día en Loo. [SEP]',\n '[CLS] Es un día en Loe. [SEP]',\n '[CLS] Es un día en Loas. [SEP]',\n '[CLS] Es un día en Loan. [SEP]',\n '[CLS] Es un día en lian. [SEP]',\n '[CLS] Es un día en lián. [SEP]',\n '[CLS] Es un día en liano. [SEP]',\n '[CLS] Es un día en liar. [SEP]',\n '[CLS] Es un día en lia. [SEP]',\n '[CLS] Es un día en poa. [SEP]',\n '[CLS] Es un día en poo. [SEP]',\n '[CLS] Es un día en poas. [SEP]',\n '[CLS] Es un día en pou. [SEP]',\n '[CLS] Es un día en poar. [SEP]',\n '[CLS] Es un día en llar. [SEP]',\n '[CLS] Es un día en llr. [SEP]',\n '[CLS] Es un día en llal. [SEP]',\n '[CLS] Es un día en llara. [SEP]',\n '[CLS] Es un día en llare. [SEP]',\n '[CLS] Es un día para llare. [SEP]',\n '[CLS] Es un día para llar. [SEP]',\n '[CLS] Es un día para llate. [SEP]',\n '[CLS] Es un día para llari. [SEP]',\n '[CLS] Es un día para llaar. [SEP]',\n '[CLS] Es un día para palaar. [SEP]',\n '[CLS] Es un día para pallo. [SEP]',\n '[CLS] Es un día para paloo. [SEP]',\n '[CLS] Es un día para palar. [SEP]',\n '[CLS] Es un día para palla. [SEP]',\n '[CLS] Es un día para lla. [SEP]',\n '[CLS] Es un día para llo. [SEP]',\n '[CLS] Es un día para lle. [SEP]',\n '[CLS] Es un día para ll. [SEP]',\n '[CLS] Es un día para llos. [SEP]',\n '[CLS] Es un día para lolos. [SEP]',\n '[CLS] Es un día para lolo. [SEP]',\n '[CLS] Es un día para loos. [SEP]',\n '[CLS] Es un día para loles. [SEP]',\n '[CLS] Es un día para lolas. [SEP]',\n '[CLS] Es un día para calllas. [SEP]',\n '[CLS] Es un día para calllos. [SEP]',\n '[CLS] Es un día para callas. [SEP]',\n '[CLS] Es un día para callla. [SEP]',\n '[CLS] Es un día para callles. [SEP]',\n '[CLS] Es un día. callles. [SEP]',\n '[CLS] Es un día. calllos. [SEP]',\n '[CLS] Es un día. calllas. [SEP]',\n '[CLS] Es un día. callle. [SEP]',\n '[CLS] Es un día. callos. [SEP]',\n '[CLS] Es un día. palos. [SEP]',\n '[CLS] Es un día. palo. [SEP]',\n '[CLS] Es un día. pallos. [SEP]',\n '[CLS] Es un día. pals. [SEP]',\n '[CLS] Es un día. palmos. [SEP]',\n '[CLS] Es un día. salmos. [SEP]',\n '[CLS] Es un día. salos. [SEP]',\n '[CLS] Es un día. salmo. [SEP]',\n '[CLS] Es un día. sal sal. [SEP]',\n '[CLS] Es un día. salmón. [SEP]',\n '[CLS] Es un día. bolmón. [SEP]',\n '[CLS] Es un día. bolón. [SEP]',\n '[CLS] Es un día. bollón. [SEP]',\n '[CLS] Es un día. bolín. [SEP]',\n '[CLS] Es un día. boltón. [SEP]',\n '[CLS] Es un día. rosatón. [SEP]',\n '[CLS] Es un día. rosaton. [SEP]',\n '[CLS] Es un día. rosato. [SEP]',\n '[CLS] Es un día. rosaón. [SEP]',\n '[CLS] Es un día. rosa.. [SEP]',\n '[CLS] Es un día a rosa.. [SEP]',\n '[CLS] Es un día a rosa,. [SEP]',\n '[CLS] Es un día a rosa y. [SEP]',\n '[CLS] Es un día a rosa.... [SEP]',\n '[CLS] Es un día a rosa :. [SEP]',\n '[CLS] Es un día a Rosa :. [SEP]',\n '[CLS] Es un día a Rosa.. [SEP]',\n '[CLS] Es un día a Rosa.... [SEP]',\n '[CLS] Es un día a Rosa,. [SEP]',\n '[CLS] Es un día a Rosa y. [SEP]',\n '[CLS] Es un día a rose y. [SEP]',\n '[CLS] Es un día a roses. [SEP]',\n '[CLS] Es un día a rose e. [SEP]',\n '[CLS] Es un día a rose.. [SEP]',\n '[CLS] Es un día a rose a. [SEP]',\n '[CLS] Es un día a la a. [SEP]',\n '[CLS] Es un día a la d. [SEP]',\n '[CLS] Es un día a la de. [SEP]',\n '[CLS] Es un día a la an. [SEP]',\n '[CLS] Es un día a la o. [SEP]',\n '[CLS] Es un día a. o. [SEP]',\n '[CLS] Es un día a. a. [SEP]',\n '[CLS] Es un día a. e. [SEP]',\n '[CLS] Es un día a. s. [SEP]',\n '[CLS] Es un día a. u. [SEP]']"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "nextSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}