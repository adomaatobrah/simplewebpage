{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "#device = 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>>fr<<, >>es<<, >>it<<, >>pt<<, >>pt_br<<, >>ro<<, >>ca<<, >>gl<<, >>pt_BR<<, >>la<<, >>wa<<, >>fur<<, >>oc<<, >>fr_CA<<, >>sc<<, >>es_ES<<, >>es_MX<<, >>es_AR<<, >>es_PR<<, >>es_UY<<, >>es_CL<<, >>es_CO<<, >>es_CR<<, >>es_GT<<, >>es_HN<<, >>es_NI<<, >>es_PA<<, >>es_PE<<, >>es_VE<<, >>es_DO<<, >>es_EC<<, >>es_SV<<, >>an<<, >>pt_PT<<, >>frp<<, >>lad<<, >>vec<<, >>fr_FR<<, >>co<<, >>it_IT<<, >>lld<<, >>lij<<, >>lmo<<, >>nap<<, >>rm<<, >>scn<<, >>mwl<<'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "en_ROMANCE_model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
    "en_ROMANCE_tokenizer = MarianTokenizer.from_pretrained(en_ROMANCE_model_name)\n",
    "', '.join(en_ROMANCE_tokenizer.supported_language_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ROMANCE = MarianMTModel.from_pretrained(en_ROMANCE_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMANCE_en_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "ROMANCE_en_tokenizer = MarianTokenizer.from_pretrained(ROMANCE_en_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROMANCE_en = MarianMTModel.from_pretrained(ROMANCE_en_model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monkey_patch(model, new_postproc_fn):\n",
    "    cls = model.__class__\n",
    "    func_name = \"postprocess_next_token_scores\"\n",
    "    orig_name = \"_orig_\" + func_name\n",
    "    if not hasattr(cls, orig_name):\n",
    "        setattr(cls, orig_name, getattr(cls, func_name))\n",
    "    setattr(cls, func_name, new_postproc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_next_token_scores(self, scores, input_ids, *a, **kw):\n",
    "    print(input_ids.shape, scores.shape)\n",
    "    batch_size, vocab_size = scores.shape\n",
    "    cur_len = input_ids.shape[1]\n",
    "    for hypothesis_idx in range(batch_size):\n",
    "        cur_hypothesis = input_ids[hypothesis_idx]\n",
    "        print(en_ROMANCE_tokenizer.convert_ids_to_tokens(cur_hypothesis))\n",
    "\n",
    "    # Hack the beam\n",
    "    if cur_len == 2:\n",
    "        force_token_id = 1886 # cor\n",
    "        #force_token_id = 3675 # sal\n",
    "        #print(scores[:, force_token_id])\n",
    "        self._force_token_ids_generation(scores, token_ids=[force_token_id])\n",
    "\n",
    "    print(scores[:, self.config.eos_token_id])\n",
    "    return self._orig_postprocess_next_token_scores(scores, input_ids, *a, **kw)\n",
    "\n",
    "monkey_patch(en_ROMANCE, postprocess_next_token_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1]) torch.Size([10, 65001])\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "['<pad>']\n",
      "tensor([-7.2095, -7.2095, -7.2095, -7.2095, -7.2095, -7.2095, -7.2095, -7.2095,\n",
      "        -7.2095, -7.2095], device='cuda:0')\n",
      "torch.Size([10, 2]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Corr']\n",
      "['<pad>', '▁Corri']\n",
      "['<pad>', '▁Corre']\n",
      "['<pad>', '▁corri']\n",
      "['<pad>', '▁Cor']\n",
      "['<pad>', '▁He']\n",
      "['<pad>', '▁Yo']\n",
      "['<pad>', '▁Me']\n",
      "['<pad>', '▁Fu']\n",
      "['<pad>', '▁Hu']\n",
      "tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "       device='cuda:0')\n",
      "torch.Size([10, 3]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Yo', '▁cor']\n",
      "['<pad>', '▁Me', '▁cor']\n",
      "['<pad>', '▁He', '▁cor']\n",
      "['<pad>', '▁Corre', '▁cor']\n",
      "['<pad>', '▁Corri', '▁cor']\n",
      "['<pad>', '▁corri', '▁cor']\n",
      "['<pad>', '▁Corr', '▁cor']\n",
      "['<pad>', '▁Cor', '▁cor']\n",
      "['<pad>', '▁Fu', '▁cor']\n",
      "['<pad>', '▁Hu', '▁cor']\n",
      "tensor([-11.8194, -13.3916, -10.4630, -12.4498, -12.1174, -12.2610, -12.9104,\n",
      "        -12.5495, -12.8968, -12.6807], device='cuda:0')\n",
      "torch.Size([10, 4]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Yo', '▁cor', 'rí']\n",
      "['<pad>', '▁Me', '▁cor', 'rí']\n",
      "['<pad>', '▁Yo', '▁cor', 'ría']\n",
      "['<pad>', '▁Yo', '▁cor', 'ré']\n",
      "['<pad>', '▁He', '▁cor', 'rido']\n",
      "['<pad>', '▁Yo', '▁cor', 'r']\n",
      "['<pad>', '▁Yo', '▁cor', 'í']\n",
      "['<pad>', '▁Yo', '▁cor', 'rÃ']\n",
      "['<pad>', '▁He', '▁cor', 'rí']\n",
      "['<pad>', '▁Yo', '▁cor', 'ria']\n",
      "tensor([-8.6730, -8.8445, -8.5174, -8.6985, -8.7284, -8.4341, -8.7334, -7.2793,\n",
      "        -8.7666, -8.4454], device='cuda:0')\n",
      "torch.Size([10, 5]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y']\n",
      "['<pad>', '▁Me', '▁cor', 'rí', '▁y']\n",
      "['<pad>', '▁Yo', '▁cor', 'ría', '▁y']\n",
      "['<pad>', '▁Yo', '▁cor', 'ré', '▁y']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', ',']\n",
      "['<pad>', '▁He', '▁cor', 'rido', '▁y']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '...']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'í', '▁y']\n",
      "['<pad>', '▁He', '▁cor', 'rí', '▁y']\n",
      "tensor([ -9.1978,  -9.5832, -10.1460,  -9.4057,  -7.4680,  -9.3735,  -4.3652,\n",
      "         -1.7586,  -9.3156,  -9.4003], device='cuda:0')\n",
      "torch.Size([10, 6]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal']\n",
      "['<pad>', '▁Me', '▁cor', 'rí', '▁y', '▁sal']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁me']\n",
      "['<pad>', '▁Yo', '▁cor', 'ría', '▁y', '▁sal']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sa']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁salto']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁saltar']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁Sal']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁pul']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁he']\n",
      "tensor([ -8.9496,  -8.9792,  -8.8702,  -8.8939, -12.1253,  -4.5046,  -4.3799,\n",
      "         -9.1090,  -9.0160,  -9.0818], device='cuda:0')\n",
      "torch.Size([10, 7]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té']\n",
      "['<pad>', '▁Me', '▁cor', 'rí', '▁y', '▁sal', 'té']\n",
      "['<pad>', '▁Yo', '▁cor', 'ría', '▁y', '▁sal', 'té']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sa', 'lí']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁salto', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁Sal', 'té']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁me', '▁sal']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁pul', 'é']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁saltar', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁he', '▁sal']\n",
      "tensor([-4.3344, -4.5631, -4.2038, -4.9492, -0.1108, -3.9145, -8.9024, -4.1108,\n",
      "        -0.1105, -8.6673], device='cuda:0')\n",
      "torch.Size([10, 8]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.']\n",
      "['<pad>', '▁Me', '▁cor', 'rí', '▁y', '▁sal', 'té', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'ría', '▁y', '▁sal', 'té', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sa', 'lí', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁me', '▁sal', 'té']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁Sal', 'té', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁pul', 'é', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁he', '▁sal', 'tado']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '▁a']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '...']\n",
      "tensor([-0.1099, -0.1106, -0.1088, -0.1111, -4.1166, -0.1104, -0.1098, -4.0381,\n",
      "        -7.7640, -0.1371], device='cuda:0')\n",
      "torch.Size([10, 9]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁me', '▁sal', 'té', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁he', '▁sal', 'tado', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '▁a', '▁la']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '▁a', '▁saltar']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁¿']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁-']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁No']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁¡']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '▁a', '▁los']\n",
      "tensor([-0.1086, -0.1076, -8.9176, -3.9878, -7.2717, -5.7745, -6.7611, -6.2877,\n",
      "        -0.1473, -8.6697], device='cuda:0')\n",
      "torch.Size([10, 10]) torch.Size([10, 65001])\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '▁a', '▁saltar', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '▁a', '▁la', '▁calle']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁¿', 'Qué']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '▁a', '▁la', '▁mierda']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁-', '▁¿']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁No', '.']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁¿', 'Por']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁¿', 'Cómo']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '.', '▁No', ',']\n",
      "['<pad>', '▁Yo', '▁cor', 'rí', '▁y', '▁sal', 'té', '▁a', '▁la', '▁puerta']\n",
      "tensor([ -0.1065,  -4.2305,  -8.7889,  -4.2240,  -7.4106,  -0.1285, -10.8489,\n",
      "         -8.5961,  -5.3407,  -4.0586], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Yo corrí y salté.',\n",
       " 'Me corrí y salté.',\n",
       " 'Yo corría y salté.',\n",
       " 'Yo corrí y me salté.',\n",
       " 'Yo corrí y he saltado.',\n",
       " 'Yo corrí y salí.',\n",
       " 'Yo corrí y Salté.',\n",
       " 'Yo corrí y pulé.',\n",
       " 'Yo corrí y salto.',\n",
       " 'Yo corrí y salté a saltar.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(tokenizer, model, text, num_outputs):\n",
    "    \"\"\"Use beam search to get a reasonable translation of 'text'\"\"\"\n",
    "    # Tokenize the source text\n",
    "    tokenizer.current_spm = tokenizer.spm_source ### HACK!\n",
    "    batch = tokenizer.prepare_translation_batch([text]).to(model.device)\n",
    "    \n",
    "    # Run model\n",
    "    num_beams = num_outputs\n",
    "    translated = model.generate(**batch, num_beams=num_beams, num_return_sequences=num_outputs, max_length=128)\n",
    "    \n",
    "    # Untokenize the output text.\n",
    "    tokenizer.current_spm = tokenizer.spm_target\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False) for t in translated]\n",
    "\n",
    "translate(en_ROMANCE_tokenizer, en_ROMANCE, \">>es<< I ran and I jumped.\", 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"swish\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"MarianMTModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      65000\n",
       "    ]\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_attention_heads\": 8,\n",
       "  \"decoder_ffn_dim\": 2048,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 65000,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 8,\n",
       "  \"encoder_ffn_dim\": 2048,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"extra_pos_embeddings\": 65001,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_length\": 512,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": false,\n",
       "  \"num_beams\": 6,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 65000,\n",
       "  \"scale_embedding\": true,\n",
       "  \"static_position_embeddings\": true,\n",
       "  \"vocab_size\": 65001\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ROMANCE.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0men_ROMANCE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_logits_during_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0madjust_logits_during_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_force_token_ids_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/code/transformers/src/transformers/modeling_marian.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_ROMANCE.adjust_logits_during_generation??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0men_ROMANCE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_force_token_ids_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0m_force_token_ids_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mall_but_token_ids_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scores should be of rank 2 with shape: [batch_size, vocab_size]\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_but_token_ids_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/code/transformers/src/transformers/modeling_bart.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_ROMANCE._force_token_ids_generation??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ROMANCE_tokenizer.current_spm = en_ROMANCE_tokenizer.spm_target\n",
    "tokens = en_ROMANCE_tokenizer.tokenize(\"Yo salté.\")\n",
    "list(zip(en_ROMANCE_tokenizer.convert_tokens_to_ids(tokens), tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ROMANCE.postprocess_next_token_scores??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer2020",
   "language": "python",
   "name": "summer2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
