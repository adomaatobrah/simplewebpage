{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://nonint.com/2020/03/27/fine-tuning-xlnet-for-generation-tasks/ (though I suspect the approach is fundamentally flawed...)\n",
    "* https://huggingface.co/transformers/model_doc/xlnet.html\n",
    "* Encoder-decoder architecture blog: https://medium.com/huggingface/encoder-decoders-in-transformers-a-hybrid-pre-trained-architecture-for-seq2seq-af4d7bf14bb8\n",
    "* https://github.com/huggingface/transformers/pull/5522/files (for train/finetune xlnet)\n",
    "* https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_xlnet.py\n",
    "\n",
    "Distantly related:\n",
    "* T5 https://arxiv.org/abs/1910.10683\n",
    "* https://www.microsoft.com/en-us/research/blog/introducing-mass-a-pre-training-method-that-outperforms-bert-and-gpt-in-sequence-to-sequence-language-generation-tasks/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'cuda'"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLNetTokenizer, XLNetConfig, XLNetLMHeadModel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "model = XLNetLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLNet supposedly needs \"padding text\" to make inference make sense (see docs)... TODO look into why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, my dog is very cute\"\n",
    "# Use indices from end because of that crazy padding text.\n",
    "tok_idx_from_end = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'▁dog'"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(PADDING_TEXT + \" \" + text, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "n_tokens = input_ids.shape[1]\n",
    "idx_to_predict = n_tokens - tok_idx_from_end\n",
    "tokenizer.convert_ids_to_tokens(input_ids[0, idx_to_predict].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we'd define a `labels` to calculate a loss. We won't actually use it right now though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 1])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "labels = input_ids[:, idx_to_predict].unsqueeze(0)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the \"permutation mask\": which tokens does each token get to look at when making the prediction?\n",
    "\n",
    "It's `batch_size` by `src_token` by `tgt_token`. Element `b, src, tgt` is `0` if `tgt` gets to see `src` in batch sample `b`, `1` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0., 1., 0., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [0., 1., 0., 0., 0.]]], device='cuda:0')"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "perm_mask = torch.zeros((1, n_tokens, n_tokens), dtype=torch.float, device=device)\n",
    "# Mask the token to predict.\n",
    "perm_mask[:, :, idx_to_predict] = 1.0\n",
    "perm_mask[:, -5:, -5:]  # show it for the last 5 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the \"target mapping\", which is `batch_size` x `num_targets` x `seq_length`. Each target should have a `1` for one of the sequence elements, corresponding to which token to try to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n          0., 0., 0.]]], device='cuda:0')"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)  # Shape [1, 1, seq_length] => let's predict one token\n",
    "target_mapping[0, 0, idx_to_predict] = 1.0  # Our first (and only) prediction\n",
    "target_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the model. It returns the next token logits, and other data if we ask but we didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([[[ -2.7657, -11.9402, -11.9371,  ...,  -9.3074,  -9.3252, -10.9300]]],\n        device='cuda:0'),)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for filling in that blank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['▁name',\n '▁dog',\n '▁avatar',\n '▁cat',\n '▁son',\n '▁blog',\n '▁picture',\n '▁friend',\n '▁daughter',\n '▁boy',\n '▁wife',\n '▁baby',\n '▁brother',\n '▁puppy',\n '▁face',\n '▁girl',\n '▁husband',\n '▁hair',\n '▁website',\n '▁boyfriend',\n '▁sister',\n '▁nephew',\n '▁guy',\n '▁kitten',\n '▁character',\n '▁site',\n '▁child',\n '▁photo',\n '▁post',\n '▁girlfriend']"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(next_token_logits[0, 0].topk(30).indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check for yourself that the predictions don't change if you change the \"masked\" token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}