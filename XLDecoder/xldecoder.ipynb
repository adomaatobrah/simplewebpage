{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import XLNetTokenizer, XLNetConfig\n",
    "from transformers import modeling_xlnet, modeling_bart\n",
    "from transformers.testing_utils import require_multigpu, require_torch, slow, torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch_device\n",
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "encoder_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "encoder_tokenizer = MarianTokenizer.from_pretrained(encoder_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = MarianMTModel.from_pretrained(encoder_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_hidden_states': tensor([[[-0.1111, -0.0358, -0.3962,  ..., -0.2749,  0.2238,  0.2691],\n",
       "          [-0.1629,  0.0071, -0.1031,  ..., -0.0354, -0.0248, -0.1916],\n",
       "          [-0.0050, -0.0080, -0.0393,  ..., -0.1553, -0.0402,  0.1821]]]),\n",
       " 'padding_mask': tensor([[False, False, False]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_L2(src):\n",
    "    \"\"\"\n",
    "    Return the encoder hidden states and the padding mask (1 if the token is padding)\"\"\"\n",
    "    batch = encoder_tokenizer.prepare_translation_batch([src]).to(device)\n",
    "    with torch.no_grad():\n",
    "        return dict(\n",
    "            encoder_hidden_states=encoder_model.get_encoder()(**batch).last_hidden_state,\n",
    "            padding_mask=batch['attention_mask'].eq(0) # invert the attention mask, which is 1 if we *should* include this token.\n",
    "        )\n",
    "encoder_output = encode_L2(\"Buenos días\")\n",
    "encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 9653,  2851,     3,     0],\n",
       "        [  143,     0, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_tokenizer.prepare_translation_batch([\"Buenos días.\", \"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on paper and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-stream attention \n",
    "\n",
    "* `g` represents the **query**: it gets to look at all the prior words and the current desired position embedding, but *not* the content that's there.\n",
    "* `h` represents the **content**: it gets everything that *g* does but also the content that's there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a custom XLNet that adds attention to encoder inputs.\n",
    "\n",
    "Thanks to the wonders of inheritance, I can reuse most of the existing code even though it wasn't designed this way. Except for the main model `forward` methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLDecoderConfig(XLNetConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_dim=None,\n",
    "        cross_attention_heads=12,\n",
    "        cross_attention_dropout=.1,\n",
    "        **kw\n",
    "    ):\n",
    "        super().__init__(**kw)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.cross_attention_heads = cross_attention_heads\n",
    "        self.cross_attention_dropout = cross_attention_dropout\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLDecoderLayer(modeling_xlnet.XLNetLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        embed_dim = config.d_model\n",
    "\n",
    "        self.encoder_attn = modeling_bart.SelfAttention(\n",
    "            embed_dim,\n",
    "            config.cross_attention_heads,\n",
    "            dropout=config.cross_attention_dropout,\n",
    "            encoder_decoder_attention=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        output_h,\n",
    "        output_g,\n",
    "        attn_mask_h,\n",
    "        attn_mask_g,\n",
    "        r,\n",
    "        seg_mat,\n",
    "        mems=None,\n",
    "        target_mapping=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "    ):\n",
    "        # Run masked relative attention on target.\n",
    "        outputs = self.rel_attn(\n",
    "            output_h,\n",
    "            output_g,\n",
    "            attn_mask_h,\n",
    "            attn_mask_g,\n",
    "            r,\n",
    "            seg_mat,\n",
    "            mems=mems,\n",
    "            target_mapping=target_mapping,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        output_h, output_g = outputs[:2]\n",
    "\n",
    "        # Cross attention, based on modeling_bart. Note: Some things about modeling_t5 seem clearer; maybe use that?\n",
    "        # Need a function because we want to do the same thing on both streams.\n",
    "        def cross_attention(x):\n",
    "            # The attention module only outputs the attention values.\n",
    "            # It does *not* add in `x`.\n",
    "            attn_out, _ = self.encoder_attn(\n",
    "                query=x,\n",
    "                key=encoder_hidden_states,\n",
    "                    key_padding_mask=encoder_attention_mask,\n",
    "                layer_state=None # ignore this optimization for now.\n",
    "            )\n",
    "            return self.encoder_attn_layer_norm(\n",
    "                x + self.dropout(attn_out))\n",
    "        \n",
    "        # If finetuning on another task, the query stream is dropped. We're not doing this now, but I'll leave the logic...\n",
    "        if output_g is not None:\n",
    "            output_g = cross_attention(output_g)\n",
    "            output_g = self.ff(output_g)\n",
    "        output_h = cross_attention(output_h)\n",
    "        output_h = self.ff(output_h)\n",
    "\n",
    "        outputs = (output_h, output_g) + outputs[2:]  # The additional output includes attention information; thread it back in.\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLDecoderModel(modeling_xlnet.XLNetModel):\n",
    "    def __init__(self, config):\n",
    "        # This does one extra step of initialization that we're about to override...\n",
    "        super().__init__(config)\n",
    "        # here. Just use XLDecoderLayer instead.\n",
    "        self.layer = nn.ModuleList([XLDecoderLayer(config) for _ in range(config.n_layer)])\n",
    "        self.encoder_to_decoder = nn.Linear(config.encoder_dim, config.d_model)\n",
    "        self.init_weights()\n",
    "    \n",
    "    # Lots of copy-paste, but gotta thread encoder data through here :(\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        mems=None,\n",
    "        perm_mask=None,\n",
    "        target_mapping=None,\n",
    "        token_type_ids=None,\n",
    "        input_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        use_cache=True,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_tuple=None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_tuple = return_tuple if return_tuple is not None else self.config.use_return_tuple\n",
    "\n",
    "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
    "        # but we want a unified interface in the library with the batch size on the first dimension\n",
    "        # so we move here the first dimension (batch) to the end\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_ids = input_ids.transpose(0, 1).contiguous()\n",
    "            qlen, bsz = input_ids.shape[0], input_ids.shape[1]\n",
    "        elif inputs_embeds is not None:\n",
    "            inputs_embeds = inputs_embeds.transpose(0, 1).contiguous()\n",
    "            qlen, bsz = inputs_embeds.shape[0], inputs_embeds.shape[1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        token_type_ids = token_type_ids.transpose(0, 1).contiguous() if token_type_ids is not None else None\n",
    "        input_mask = input_mask.transpose(0, 1).contiguous() if input_mask is not None else None\n",
    "        attention_mask = attention_mask.transpose(0, 1).contiguous() if attention_mask is not None else None\n",
    "        perm_mask = perm_mask.permute(1, 2, 0).contiguous() if perm_mask is not None else None\n",
    "        target_mapping = target_mapping.permute(1, 2, 0).contiguous() if target_mapping is not None else None\n",
    "\n",
    "        mlen = mems[0].shape[0] if mems is not None and mems[0] is not None else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        dtype_float = self.dtype\n",
    "        device = self.device\n",
    "\n",
    "        # Attention mask\n",
    "        # causal attention mask\n",
    "        if self.attn_type == \"uni\":\n",
    "            attn_mask = self.create_mask(qlen, mlen)\n",
    "            attn_mask = attn_mask[:, :, None, None]\n",
    "        elif self.attn_type == \"bi\":\n",
    "            attn_mask = None\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported attention type: {}\".format(self.attn_type))\n",
    "\n",
    "        # data mask: input mask & perm mask\n",
    "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \"\n",
    "        \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
    "        if input_mask is None and attention_mask is not None:\n",
    "            input_mask = 1.0 - attention_mask\n",
    "        if input_mask is not None and perm_mask is not None:\n",
    "            data_mask = input_mask[None] + perm_mask\n",
    "        elif input_mask is not None and perm_mask is None:\n",
    "            data_mask = input_mask[None]\n",
    "        elif input_mask is None and perm_mask is not None:\n",
    "            data_mask = perm_mask\n",
    "        else:\n",
    "            data_mask = None\n",
    "\n",
    "        if data_mask is not None:\n",
    "            # all mems can be attended to\n",
    "            if mlen > 0:\n",
    "                mems_mask = torch.zeros([data_mask.shape[0], mlen, bsz]).to(data_mask)\n",
    "                data_mask = torch.cat([mems_mask, data_mask], dim=1)\n",
    "            if attn_mask is None:\n",
    "                attn_mask = data_mask[:, :, :, None]\n",
    "            else:\n",
    "                attn_mask += data_mask[:, :, :, None]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = (attn_mask > 0).to(dtype_float)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            non_tgt_mask = -torch.eye(qlen).to(attn_mask)\n",
    "            if mlen > 0:\n",
    "                non_tgt_mask = torch.cat([torch.zeros([qlen, mlen]).to(attn_mask), non_tgt_mask], dim=-1)\n",
    "            non_tgt_mask = ((attn_mask + non_tgt_mask[:, :, None, None]) > 0).to(attn_mask)\n",
    "        else:\n",
    "            non_tgt_mask = None\n",
    "\n",
    "        # Word embeddings and prepare h & g hidden states\n",
    "        if inputs_embeds is not None:\n",
    "            word_emb_k = inputs_embeds\n",
    "        else:\n",
    "            word_emb_k = self.word_embedding(input_ids)\n",
    "        output_h = self.dropout(word_emb_k)\n",
    "        if target_mapping is not None:\n",
    "            word_emb_q = self.mask_emb.expand(target_mapping.shape[0], bsz, -1)\n",
    "            # else:  # We removed the inp_q input which was same as target mapping\n",
    "            #     inp_q_ext = inp_q[:, :, None]\n",
    "            #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
    "            output_g = self.dropout(word_emb_q)\n",
    "        else:\n",
    "            output_g = None\n",
    "\n",
    "        # Segment embedding\n",
    "        if token_type_ids is not None:\n",
    "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
    "            if mlen > 0:\n",
    "                mem_pad = torch.zeros([mlen, bsz], dtype=torch.long, device=device)\n",
    "                cat_ids = torch.cat([mem_pad, token_type_ids], dim=0)\n",
    "            else:\n",
    "                cat_ids = token_type_ids\n",
    "\n",
    "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
    "            seg_mat = (token_type_ids[:, None] != cat_ids[None, :]).long()\n",
    "            seg_mat = F.one_hot(seg_mat, num_classes=2).to(dtype_float)\n",
    "        else:\n",
    "            seg_mat = None\n",
    "\n",
    "        # Positional encoding\n",
    "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz)\n",
    "        pos_emb = self.dropout(pos_emb)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
    "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
    "        if head_mask is not None:\n",
    "            if head_mask.dim() == 1:\n",
    "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
    "            elif head_mask.dim() == 2:\n",
    "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            head_mask = head_mask.to(\n",
    "                dtype=next(self.parameters()).dtype\n",
    "            )  # switch to fload if need + fp16 compatibility\n",
    "        else:\n",
    "            head_mask = [None] * self.n_layer\n",
    "\n",
    "            \n",
    "        # transform encoder data.\n",
    "        encoder_hidden_states = self.encoder_to_decoder(encoder_hidden_states)\n",
    "\n",
    "        new_mems = ()\n",
    "        if mems is None:\n",
    "            mems = [None] * len(self.layer)\n",
    "\n",
    "        attentions = [] if output_attentions else None\n",
    "        hidden_states = [] if output_hidden_states else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.mem_len is not None and self.mem_len > 0 and use_cache is True:\n",
    "                # cache new mems\n",
    "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
    "            if output_hidden_states:\n",
    "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
    "\n",
    "            outputs = layer_module(\n",
    "                output_h,\n",
    "                output_g,\n",
    "                attn_mask_h=non_tgt_mask,\n",
    "                attn_mask_g=attn_mask,\n",
    "                r=pos_emb,\n",
    "                seg_mat=seg_mat,\n",
    "                mems=mems[i],\n",
    "                target_mapping=target_mapping,\n",
    "                head_mask=head_mask[i],\n",
    "                output_attentions=output_attentions,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "            )\n",
    "            output_h, output_g = outputs[:2]\n",
    "            if output_attentions:\n",
    "                attentions.append(outputs[2])\n",
    "\n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
    "\n",
    "        output = self.dropout(output_g if output_g is not None else output_h)\n",
    "\n",
    "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
    "        output = output.permute(1, 0, 2).contiguous()\n",
    "\n",
    "        # TODO Teven: fix this test to only use use_cache.\n",
    "        if not (self.mem_len is not None and self.mem_len > 0 and use_cache is True):\n",
    "            new_mems = None\n",
    "\n",
    "        if output_hidden_states:\n",
    "            if output_g is not None:\n",
    "                hidden_states = tuple(h.permute(1, 0, 2).contiguous() for hs in hidden_states for h in hs)\n",
    "            else:\n",
    "                hidden_states = tuple(hs.permute(1, 0, 2).contiguous() for hs in hidden_states)\n",
    "\n",
    "        if output_attentions:\n",
    "            if target_mapping is not None:\n",
    "                # when target_mapping is provided, there are 2-tuple of attentions\n",
    "                attentions = tuple(\n",
    "                    tuple(att_stream.permute(2, 3, 0, 1).contiguous() for att_stream in t) for t in attentions\n",
    "                )\n",
    "            else:\n",
    "                attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions)\n",
    "\n",
    "        if return_tuple:\n",
    "            return tuple(v for v in [output, new_mems, hidden_states, attentions] if v is not None)\n",
    "\n",
    "        return modeling_xlnet.XLNetModelOutput(\n",
    "            last_hidden_state=output, mems=new_mems, hidden_states=hidden_states, attentions=attentions\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLDecoderLMHeadModel(modeling_xlnet.XLNetLMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = XLDecoderModel(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    # Yucky copy-paste again.\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        mems=None,\n",
    "        perm_mask=None,\n",
    "        target_mapping=None,\n",
    "        token_type_ids=None,\n",
    "        input_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=True,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_tuple=None,\n",
    "    ):\n",
    "        return_tuple = return_tuple if return_tuple is not None else self.config.use_return_tuple\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_tuple=return_tuple,\n",
    "        )\n",
    "\n",
    "        logits = self.lm_loss(transformer_outputs[0])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        if return_tuple:\n",
    "            output = (logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return modeling_xlnet.XLNetLMHeadModelOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            mems=transformer_outputs.mems,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = XLDecoderConfig.from_pretrained(model_name, mem_len=1024, encoder_dim=encoder_model.config.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.mem_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLDecoderLMHeadModel(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked inputs get no gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that [input and output embedding weights are tied](https://github.com/huggingface/transformers/blob/015dc51fe35f758b70a3066298dff6ae917c11ba/src/transformers/modeling_utils.py#L351). So just checking the word embedding gradient doesn't suffice if it's a predicted output. So... Untie the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_loss_weight = torch.zeros_like(model.transformer.word_embedding.weight)\n",
    "lm_loss_weight.copy_(model.transformer.word_embedding.weight)\n",
    "model.lm_loss.weight = torch.nn.Parameter(lm_loss_weight)\n",
    "assert model.lm_loss.weight is not model.transformer.word_embedding.weight\n",
    "assert torch.allclose(model.lm_loss.weight, model.transformer.word_embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, my dog is very cute\"\n",
    "tok_idx_from_end = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁dog'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors='pt').to(torch_device)\n",
    "n_tokens = input_ids.shape[1]\n",
    "idx_to_predict = n_tokens - tok_idx_from_end\n",
    "tokenizer.convert_ids_to_tokens(input_ids[0, idx_to_predict].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we'd define a `labels` to calculate a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = input_ids[:, idx_to_predict].unsqueeze(0)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the \"permutation mask\": which tokens does each token get to look at when making the prediction?\n",
    "\n",
    "It's `batch_size` by `src_token` by `tgt_token`. Element `b, src, tgt` is `0` if `tgt` gets to see `src` in batch sample `b`, `1` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0.]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_mask = torch.zeros((1, n_tokens, n_tokens), dtype=torch.float, device=device)\n",
    "# Mask the token to predict.\n",
    "perm_mask[:, :, idx_to_predict] = 1.0\n",
    "\n",
    "# Mask the one after it too, just to illustrate.\n",
    "perm_mask[:, :, idx_to_predict+1] = 1.0\n",
    "perm_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the \"target mapping\", which is `batch_size` x `num_targets` x `seq_length`. Each target should have a `1` for one of the sequence elements, corresponding to which token to try to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 1., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)  # Shape [1, 1, seq_length] => let's predict one token\n",
    "target_mapping[0, 0, idx_to_predict] = 1.0  # Our first (and only) prediction\n",
    "target_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define initially empty memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mems = tuple([torch.zeros(0, 1, model.config.d_model) for _ in range(model.config.n_layer)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the model. It returns the loss and next token logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8804, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "outputs = model(\n",
    "    input_ids,\n",
    "    perm_mask=perm_mask, target_mapping=target_mapping,\n",
    "    labels=labels,\n",
    "    output_attentions=True, mems=mems,\n",
    "    encoder_hidden_states=encoder_output['encoder_hidden_states'],\n",
    "    encoder_attention_mask=encoder_output['padding_mask']\n",
    ")\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run backward and ensure that neither masked token gets any update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.1994e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0000e+00]),\n",
       " array([0.       , 0.7647348, 1.5294696, 2.2942045, 3.0589392, 3.823674 ,\n",
       "        4.588409 , 5.3531437, 6.1178784, 6.882613 , 7.647348 ],\n",
       "       dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradient_mags_by_word = np.linalg.norm(model.transformer.word_embedding.weight.grad.numpy(), axis=1)\n",
    "plt.hist(gradient_mags_by_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 11368, 19, 94, 2288, 27, 172, 10920]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.0816526, 7.117165 , 7.647348 , 7.400329 , 0.       , 0.       ,\n",
       "       7.544651 , 7.1951604], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_mags_by_word[input_ids[0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gradient_mags_by_word[input_ids[0, idx_to_predict]] == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gradient_mags_by_word[input_ids[0, idx_to_predict - 1]] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mems` is a tuple of memory states per layer. Each seems to be shape `len` x `batch`? x `config.d_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 768])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.mems[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source attention does get gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.transformer.encoder_to_decoder.weight.grad.norm() > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.transformer.layer[0].encoder_attn.k_proj.weight.grad.norm() > 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.layer[0].encoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0138)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.layer[0].encoder_attn.k_proj.weight.grad.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5781)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.layer[0].encoder_attn.v_proj.weight.grad.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0137)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.layer[0].encoder_attn.q_proj.weight.grad.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5904)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.layer[0].encoder_attn.out_proj.weight.grad.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer2020",
   "language": "python",
   "name": "summer2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
