{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>>fr<<', '>>es<<', '>>it<<', '>>pt<<', '>>pt_br<<', '>>ro<<', '>>ca<<', '>>gl<<', '>>pt_BR<<', '>>la<<', '>>wa<<', '>>fur<<', '>>oc<<', '>>fr_CA<<', '>>sc<<', '>>es_ES<<', '>>es_MX<<', '>>es_AR<<', '>>es_PR<<', '>>es_UY<<', '>>es_CL<<', '>>es_CO<<', '>>es_CR<<', '>>es_GT<<', '>>es_HN<<', '>>es_NI<<', '>>es_PA<<', '>>es_PE<<', '>>es_VE<<', '>>es_DO<<', '>>es_EC<<', '>>es_SV<<', '>>an<<', '>>pt_PT<<', '>>frp<<', '>>lad<<', '>>vec<<', '>>fr_FR<<', '>>co<<', '>>it_IT<<', '>>lld<<', '>>lij<<', '>>lmo<<', '>>nap<<', '>>rm<<', '>>scn<<', '>>mwl<<']\n"
     ]
    }
   ],
   "source": [
    "en_ROMANCE_model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
    "en_ROMANCE_tokenizer = MarianTokenizer.from_pretrained(en_ROMANCE_model_name)\n",
    "print(en_ROMANCE_tokenizer.supported_language_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ROMANCE = MarianMTModel.from_pretrained(en_ROMANCE_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMANCE_en_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "ROMANCE_en_tokenizer = MarianTokenizer.from_pretrained(ROMANCE_en_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMANCE_en = MarianMTModel.from_pretrained(ROMANCE_en_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yep\n",
      "tensor(42)\n",
      "10\n",
      " The night was dark and stormy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.2042940987481"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip = \"true\"\n",
    "english = \"It was a dark and stormy night\"\n",
    "start = \"The\"\n",
    "\n",
    "#english only- spanish behind the scenes\n",
    "if skip == \"true\":\n",
    "    tokenizer = ROMANCE_en_tokenizer\n",
    "    model = ROMANCE_en\n",
    "\n",
    "#show spanish\n",
    "else:\n",
    "    tokenizer = en_ROMANCE_tokenizer\n",
    "    model = en_ROMANCE\n",
    "\n",
    "starting_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(start))\n",
    "prefix = torch.LongTensor(starting_word)\n",
    "\n",
    "batch = tokenizer.prepare_translation_batch([\">>es<<\" + english])\n",
    "english_encoded = model.get_encoder()(**batch)\n",
    "decoder_start_token = model.config.decoder_start_token_id\n",
    "# pylint: disable=E1101\n",
    "partial_decode = torch.LongTensor([decoder_start_token]).unsqueeze(0)\n",
    "past = (english_encoded, None)\n",
    "# pylint: enable=E1101\n",
    "next_token_to_add = torch.tensor(1)\n",
    "x = 0\n",
    "\n",
    "prediction_list = []\n",
    "total = 0\n",
    "#stop when </s> token generated, or max num tokens exceded (just in case)\n",
    "while next_token_to_add.item() != 0 and x < 100:\n",
    "    model_inputs = model.prepare_inputs_for_generation(\n",
    "    partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=model.config.use_cache\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**model_inputs)\n",
    "\n",
    "    next_token_logits = model_outputs[0][:, -1, :]\n",
    "    past = model_outputs[1]\n",
    "\n",
    "    #start with user inputted beginning\n",
    "    if x < len(prefix):\n",
    "        next_token_to_add = prefix[x]\n",
    "        if next_token_to_add in next_token_logits[0].topk(1000).indices:\n",
    "            print(\"yep\") \n",
    "            print(next_token_to_add)\n",
    "            index = ((next_token_logits[0].topk(1000).indices == next_token_to_add).nonzero())[0][0].item()\n",
    "            value = next_token_logits[0].topk(1000).values\n",
    "            if x == 0:\n",
    "                save = next_token_logits\n",
    "    else:\n",
    "        next_token_to_add = next_token_logits[0].argmax()\n",
    "        total += next_token_logits[0].topk(10).values[0].item()\n",
    "        \n",
    "    #append top 10 predictions for each token to list\n",
    "    decoded_predictions = []\n",
    "    for tok in next_token_logits[0].topk(10).indices:\n",
    "        decoded_predictions.append(tokenizer.convert_ids_to_tokens(tok.item()).replace('\\u2581', '\\u00a0'))\n",
    "\n",
    "    #list of lists of predictions\n",
    "    prediction_list.append(decoded_predictions)\n",
    "\n",
    "    #add new token to tokens so far\n",
    "    partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "    x+= 1\n",
    "\n",
    "#list of tokens used to display sentence\n",
    "decoded_tokens = [sub.replace('\\u2581', '\\u00a0') for sub in tokenizer.convert_ids_to_tokens(partial_decode[0])]\n",
    "decoded_tokens.remove(\"<pad>\")\n",
    "\n",
    "final = tokenizer.decode(partial_decode[0]).replace(\"<pad>\", '')\n",
    "print(final)\n",
    "total/len(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save[0].topk(11).indices[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9219, 8.8910, 8.1260, 6.8402, 6.3231, 5.9794, 5.7846, 5.7819, 5.6579,\n",
       "        5.5030])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits[0].topk(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([9.6353, 8.0945, 5.6484, 5.5065, 4.4912, 4.0338, 3.6761, 3.4046, 3.3394,\n",
       "        3.0043]),\n",
       "indices=tensor([ 0,  3,  2, 78, 65, 39, 10, 17, 35, 23]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits[0].topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 114,  42,  20,   3,  23, 222, 229, 343, 111, 176])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits[0].topk(11).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅThe']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\xa0O', 'cto', 'pus', '\\xa0is', '\\xa0nice', '</s>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65000]])\n",
      "0 :  tensor(100)\n",
      "tensor([[65000,   100]])\n",
      "tensor([[65000,   100,  4997]])\n",
      "tensor([[65000,   100,  4997,   531]])\n",
      "tensor([[65000,   100,  4997,   531,   231]])\n",
      "tensor([[65000,   100,  4997,   531,   231, 11023]])\n",
      "tensor([[65000,   100,  4997,   531,   231, 11023,    32]])\n",
      "tensor([[65000,   100,  4997,   531,   231, 11023,    32,   805]])\n",
      "tensor([[65000,   100,  4997,   531,   231, 11023,    32,   805,  6386]])\n",
      "tensor([[65000,   100,  4997,   531,   231, 11023,    32,   805,  6386,   570]])\n",
      "tensor([[65000,   100,  4997,   531,   231, 11023,    32,   805,  6386,   570,\n",
      "          3301]])\n",
      "tensor([[65000,   100,  4997,   531,   231, 11023,    32,   805,  6386,   570,\n",
      "          3301,     3]])\n",
      " La noche era oscura y tempestuosa.\n",
      "['<pad>', '\\xa0La', '\\xa0noche', '\\xa0era', '\\xa0os', 'cura', '\\xa0y', '\\xa0tem', 'pes', 'tu', 'osa', '.', '</s>']\n",
      "[['\\xa0Era', '\\xa0Fue', '\\xa0Es', '\\xa0Ha', '\\xa0Una'], ['\\xa0noche', '\\xa0tarde', '\\xa0oscuridad', '\\xa0verdad', '\\xa0vela'], ['\\xa0era', '\\xa0fue', '\\xa0estaba', '\\xa0estuvo', '\\xa0es'], ['\\xa0os', '\\xa0som', '\\xa0tem', '\\xa0de', '\\xa0muy'], ['cura', 'cure', 'cur', 'ca', 'cu'], ['\\xa0y', ',', '.', '\\xa0e', '</s>'], ['\\xa0tem', '\\xa0tormenta', '\\xa0tor', '\\xa0os', '\\xa0tur'], ['pes', 'p', 'pra', 'pla', 'pl'], ['tu', 'ta', 'tiva', 'tada', 'table'], ['osa', 'oso', 'osas', 'ada', 'osamente'], ['.', '</s>', ',', '...', '!'], ['</s>', '.', '<pad>', '\\xa0¬ø', '\\xa0-']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The night was dark and stormy.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = en_ROMANCE_tokenizer\n",
    "model = en_ROMANCE\n",
    "#tokenizer = ROMANCE_en_tokenizer\n",
    "#model = ROMANCE_en\n",
    "\n",
    "\n",
    "english = \">>es<<It was a dark and stormy night\"\n",
    "batch = tokenizer.prepare_translation_batch([english])\n",
    "english_encoded = model.get_encoder()(**batch)\n",
    "decoder_start_token = model.config.decoder_start_token_id\n",
    "\n",
    "starting_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"La\"))\n",
    "#starting_word = tokenizer.encode(\"A la\")\n",
    "\n",
    "partial_decode = torch.LongTensor([decoder_start_token]).unsqueeze(0)\n",
    "past = (english_encoded, None)\n",
    "\n",
    "prefix = torch.LongTensor(starting_word)\n",
    "#prefix = torch.LongTensor(tokenizer.convert_tokens_to_ids(\"A la\".replace(' ', '‚ñÅ').split('‚ñÅ')))\n",
    "#prefix = torch.LongTensor(tokenizer.encode(\"A la\"))\n",
    "#prefix = torch.LongTensor(tokenizer.encode(\"A‚ñÅla‚ñÅ\"))\n",
    "next_token_to_add = torch.tensor(1)\n",
    "x = 0\n",
    "\n",
    "prediction_list = []\n",
    "\n",
    "while next_token_to_add.item() != 0 and x < 100:\n",
    "    print(partial_decode)\n",
    "    model_inputs = model.prepare_inputs_for_generation(\n",
    "    partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=model.config.use_cache\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**model_inputs)\n",
    "\n",
    "    next_token_logits = model_outputs[0][:, -1, :]\n",
    "    past = model_outputs[1]\n",
    "    \n",
    "    if x < len(prefix):\n",
    "        next_token_to_add = prefix[x]\n",
    "        print(x, \": \", next_token_to_add)\n",
    "    else:\n",
    "        next_token_to_add = next_token_logits[0].argmax()\n",
    "\n",
    "    decoded_predictions = []\n",
    "    for tok in next_token_logits[0].topk(5).indices:\n",
    "        decoded_predictions.append(tokenizer.convert_ids_to_tokens(tok.item()).replace('\\u2581', '\\u00a0'))\n",
    "    \n",
    "    prediction_list.append(decoded_predictions)\n",
    "    \n",
    "    partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "    x += 1\n",
    "    \n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(partial_decode[0])\n",
    "\n",
    "decoded_tokens = [sub.replace('\\u2581', '\\u00a0') for sub in tokenizer.convert_ids_to_tokens(partial_decode[0])] \n",
    "\n",
    "final = tokenizer.decode(partial_decode[0]).split(\"<pad>\")[1]\n",
    "print(final)\n",
    "print(decoded_tokens)\n",
    "print(prediction_list)\n",
    "\n",
    "batch2 = ROMANCE_en_tokenizer.prepare_translation_batch([\">>en<< \" + final])\n",
    "spanish_to_english = ROMANCE_en.generate(**batch2)\n",
    "new_english = ROMANCE_en_tokenizer.decode(spanish_to_english[0]).split(\"<pad>\")[1]\n",
    "\n",
    "new_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Era un noche oscureci√É¬°n y tormenta........\n",
      " Fue una oscuridad y una noche tempestuoso\n",
      " Fue una noche oscurecida y tormenta.\n",
      " Era un noche oscurecida, tempestuoso\n",
      " Era una oscuridad, y una noche tempun√°tica\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Era un noche oscureci√É¬°n y tormenta........',\n",
       " ' Fue una oscuridad y una noche tempestuoso',\n",
       " ' Fue una noche oscurecida y tormenta.',\n",
       " ' Era un noche oscurecida, tempestuoso',\n",
       " ' Era una oscuridad, y una noche tempun√°tica']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = en_ROMANCE_tokenizer\n",
    "model = en_ROMANCE\n",
    "\n",
    "english = \">>es<<It was a dark and stormy night\"\n",
    "batch = tokenizer.prepare_translation_batch([english])\n",
    "english_encoded = model.get_encoder()(**batch)\n",
    "decoder_start_token = model.config.decoder_start_token_id\n",
    "\n",
    "starting_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"\"))\n",
    "prefix = torch.LongTensor(starting_word)\n",
    "prediction_list = []\n",
    "alternatives = []\n",
    "for x in range(0, 5):\n",
    "\n",
    "    past = (english_encoded, None)\n",
    "   \n",
    "    partial_decode = torch.LongTensor([decoder_start_token]).unsqueeze(0)\n",
    "    y = 0\n",
    "    next_token_to_add = torch.tensor(1)\n",
    "    while next_token_to_add.item() != 0 and y < 100:    \n",
    "        model_inputs = model.prepare_inputs_for_generation(\n",
    "        partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=model.config.use_cache\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model(**model_inputs)\n",
    "        next_token_logits = model_outputs[0][:, -1, :]\n",
    "        past = model_outputs[1]\n",
    "\n",
    "        if y < len(prefix):\n",
    "            next_token_to_add = prefix[y]\n",
    "        else:\n",
    "            next_token_to_add = next_token_logits[0].topk(4).indices[random.randint(0, 1)]\n",
    "        decoded_predictions = []\n",
    "        for tok in next_token_logits[0].topk(5).indices:\n",
    "            decoded_predictions.append(tokenizer.convert_ids_to_tokens(tok.item()).replace('\\u2581', '\\u00a0'))\n",
    "\n",
    "        prediction_list.append(decoded_predictions)\n",
    "        partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "        y += 1\n",
    "\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(partial_decode[0])\n",
    "\n",
    "    final = tokenizer.decode(partial_decode[0]).split(\"<pad>\")[1]\n",
    "    print(final)\n",
    "    alternatives.append(final)\n",
    "    \n",
    "alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\xa0',\n",
       " '.',\n",
       " '\\xa0I',\n",
       " '\\xa0believe',\n",
       " '\\xa0this',\n",
       " '\\xa0is',\n",
       " '\\xa0the',\n",
       " '\\xa0wrong',\n",
       " '\\xa0store',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Creo que esta es la tienda equivocada.',\n",
       " ' Creo que tal vez esta es la tienda equivocada.',\n",
       " ' Creo que quiz√°s esta es la tienda equivocada.',\n",
       " ' Creo que puede que esta sea la tienda equivocada.',\n",
       " ' Creo que esta tal vez es la tienda equivocada.',\n",
       " ' Creo que quiz√° esta es la tienda equivocada.',\n",
       " ' Creo que √©sta es la tienda equivocada.',\n",
       " ' Creo que es la tienda equivocada.',\n",
       " ' Creo que esta puede ser la tienda equivocada.',\n",
       " ' Creo que quiz√° esta sea la tienda equivocada.',\n",
       " ' Creo que quiz√°s esta sea la tienda equivocada.',\n",
       " ' Creo que tal vez esta sea la tienda equivocada.',\n",
       " ' Pienso que esta es la tienda equivocada.',\n",
       " ' Creo que a lo mejor esta es la tienda equivocada.',\n",
       " ' Creo que puede que esta es la tienda equivocada.',\n",
       " ' Creo que tal vez √©sta es la tienda equivocada.',\n",
       " ' Creo que esta es una tienda equivocada.',\n",
       " ' Creo que puede que √©sta sea la tienda equivocada.',\n",
       " ' Creo que esto es la tienda equivocada.',\n",
       " ' Creo que esta tal vez sea la tienda equivocada.',\n",
       " ' Creo que tal vez es la tienda equivocada.',\n",
       " ' Creo que quiz√°s √©sta es la tienda equivocada.',\n",
       " ' Creo que esa es la tienda equivocada.',\n",
       " ' Creo que esta es la tienda incorrecta.',\n",
       " ' Creo que esta es la tienda err√≥nea.',\n",
       " ' Creo que quiz√° √©sta es la tienda equivocada.',\n",
       " ' Creo que puede ser la tienda equivocada.',\n",
       " ' Creo que esta es quiz√°s la tienda equivocada.',\n",
       " ' Creo que quiz√°s es la tienda equivocada.',\n",
       " ' Creo que tal vez esta es la tienda incorrecta.',\n",
       " ' Creo que quiz√° es la tienda equivocada.',\n",
       " ' Creo que est√° es la tienda equivocada.',\n",
       " ' Creo que esta no es la tienda correcta.',\n",
       " ' Quiz√° esta sea la tienda equivocada.',\n",
       " ' Creo que tal vez esta es la tienda err√≥nea.',\n",
       " ' Creo que esta tal vez no es la tienda correcta.',\n",
       " ' Quiz√° esta es la tienda equivocada.',\n",
       " ' Creo que es una tienda equivocada.',\n",
       " ' Creo que esta es la tienda equivocada',\n",
       " ' Creo que esta tal vez es la tienda incorrecta.',\n",
       " ' Creo que puede que esta sea la tienda incorrecta.',\n",
       " ' Creo que quiz√°s esta es la tienda incorrecta.',\n",
       " ' Creo que esta no es la tienda.',\n",
       " ' Creo que tal vez esta tienda no es la correcta.',\n",
       " ' Creo que esta tal vez no es la tienda.',\n",
       " ' Creo que quiz√° esta es la tienda incorrecta.',\n",
       " ' Creo que tal vez esta es la tienda equivocada',\n",
       " ' Creo que esta se equivoca de tienda.',\n",
       " ' Creo que esta es la tienda equivocada, tal vez.',\n",
       " ' Creo que quiz√°s esta tienda no es la correcta.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = en_ROMANCE_tokenizer\n",
    "model = en_ROMANCE\n",
    "\n",
    "english = \">>es<<I think maybe this is the wrong store.\"\n",
    "batch = tokenizer.prepare_translation_batch([english])\n",
    "                                            \n",
    "eng_to_spanish = model.generate(num_beams=50, num_return_sequences=50, **batch)\n",
    "\n",
    "translations = []\n",
    "for x in range(0, 50):\n",
    "    translations.append(tokenizer.decode(eng_to_spanish[x]).split(\"<pad>\")[1])\n",
    "\n",
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([65000,    42,  7025,    26,  2910,     3,     0, 65000, 65000, 65000,\n",
       "        65000, 65000, 65000, 65000])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_to_english[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> You're my best friend.<pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROMANCE_en_tokenizer.decode(back_to_english[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>>en<<Eres mi mejor amigo.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\">>en<<\" + tokenizer.decode(spanish[0]).replace(\"<pad> \", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([65000,   111,     7,    66,   148,   660,  2203,     3,     0, 65000,\n",
       "        65000, 65000, 65000, 65000, 65000, 65000, 65000])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_to_english[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad>You're my best friend.<pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROMANCE_en_tokenizer.decode([65000,   4932,    7,    66,   148,   660,  2203,     3,     0, 65000,\n",
    "        65000, 65000, 65000, 65000, 65000, 65000, 65000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([65000,   111,    57,   148,  7112,  9036,     0, 65000, 65000, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,     3,     0, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,   111,    57,   148,  7112,  9036,     3,     0, 65000, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,     0, 65000, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,     3,   111,     7,\n",
      "           66,   148,  7112,  9036,     3,     0])\n",
      "tensor([65000,    31,     7,    66,   148,  7112,  9036,     3,     0, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,    31,    57,   148,  7112,  9036,     0, 65000, 65000, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,    31,     7,    66,   148,  7112,  9036,     0, 65000, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,    78,   111,     7,\n",
      "           66,   148,  7112,  9036,    78,     0])\n",
      "tensor([65000,   111,     7,    66,   148,  7112, 15659,     3,     0, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,    31,     7,    66,   148,  7112,  9036,     3,   111,     7,\n",
      "           66,   148,  7112,  9036,     3,     0])\n",
      "tensor([65000,   111,   187,     2,    31,     7,    66,   148,  7112,  9036,\n",
      "            3,     0, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,     3,   111,    57,\n",
      "          148,  7112,  9036,     3,     0, 65000])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,     2,    31,     7,\n",
      "           66,   148,  7112,  9036,     3,     0])\n",
      "tensor([65000,   111,     7,    66,   148,  7112, 15659,     3,   111,     7,\n",
      "           66,   148,  7112, 15659,     3,     0])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,    78,     0, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,    31,    57,   148,  7112,  9036,     3,     0, 65000, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,     3,   111,     7,\n",
      "           66,   148,  7112, 15659,     3,     0])\n",
      "tensor([65000,   111,     7,    66,   148,  7112,  9036,     3,   111,     7,\n",
      "           66,   148,  7112,  9036,    78,     0])\n",
      "tensor([65000,   111,    57,   148,  7112, 15659,     3,     0, 65000, 65000,\n",
      "        65000, 65000, 65000, 65000, 65000, 65000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' You are my soul mate',\n",
       " \" You're my soul mate.\",\n",
       " ' You are my soul mate.',\n",
       " \" You're my soul mate\",\n",
       " \" You're my soul mate. You're my soul mate.\",\n",
       " \" you're my soul mate.\",\n",
       " ' you are my soul mate',\n",
       " \" you're my soul mate\",\n",
       " \" You're my soul mate! You're my soul mate!\",\n",
       " \" You're my soulmate.\",\n",
       " \" you're my soul mate. You're my soul mate.\",\n",
       " \" You know, you're my soul mate.\",\n",
       " \" You're my soul mate. You are my soul mate.\",\n",
       " \" You're my soul mate, you're my soul mate.\",\n",
       " \" You're my soulmate. You're my soulmate.\",\n",
       " \" You're my soul mate!\",\n",
       " ' you are my soul mate.',\n",
       " \" You're my soul mate. You're my soulmate.\",\n",
       " \" You're my soul mate. You're my soul mate!\",\n",
       " ' You are my soulmate.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = en_ROMANCE_tokenizer\n",
    "model = en_ROMANCE\n",
    "\n",
    "english = \">>es<<You're my soulmate\"\n",
    "batch = tokenizer.prepare_translation_batch([english])\n",
    "\n",
    "spanish = model.generate(**batch)\n",
    "\n",
    "batch2 = ROMANCE_en_tokenizer.prepare_translation_batch([\">>en<<\" + tokenizer.decode(spanish[0])])\n",
    "\n",
    "back_to_english = ROMANCE_en.generate(num_beams=20, num_return_sequences=20, bad_words_ids=[[1695],[1973],[7927],[55],[23],[367],[51],[12390],[1172],[45351],[39]], **batch2)\n",
    "\n",
    "translations = []\n",
    "for x in range(0, 20):\n",
    "    print(back_to_english[x])\n",
    "    translations.append(ROMANCE_en_tokenizer.decode(back_to_english[x]).replace(\"<pad>\", ''))\n",
    "    \n",
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23, 0], [51, 0]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_words_ids = [ROMANCE_en_tokenizer.encode(bad_word) for bad_word in ['-', '\"']]\n",
    "bad_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROMANCE_en_tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47075"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROMANCE_en_tokenizer.convert_tokens_to_ids('‚ô™')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. I believe this is the wrong store.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = \">>es<<I think maybe this is the wrong store.\"\n",
    "batch = tokenizer.prepare_translation_batch([english])\n",
    "                                            \n",
    "spanish = model.generate(**batch)\n",
    "\n",
    "batch = ROMANCE_en_tokenizer.prepare_translation_batch([\">>en<<\" + tokenizer.decode(spanish[0]).replace(\"<pad>\", '')])\n",
    "back_to_english = ROMANCE_en.generate(**batch)\n",
    "machine_translation = ROMANCE_en_tokenizer.decode(back_to_english[0]).replace(\"<pad>\", '')\n",
    "\n",
    "spanish_encoded = ROMANCE_en.get_encoder()(**batch)\n",
    "decoder_start_token = ROMANCE_en.config.decoder_start_token_id\n",
    "# pylint: disable=E1101\n",
    "partial_decode = torch.LongTensor([decoder_start_token]).unsqueeze(0)\n",
    "past = (spanish_encoded, None)\n",
    "# pylint: enable=E1101\n",
    "next_token_to_add = torch.tensor(1)\n",
    "x = 0\n",
    "\n",
    "prediction_list = []\n",
    "\n",
    "while next_token_to_add.item() != 0 and x < 100:\n",
    "    model_inputs = ROMANCE_en.prepare_inputs_for_generation(\n",
    "    partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=ROMANCE_en.config.use_cache\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model_outputs = ROMANCE_en(**model_inputs)\n",
    "\n",
    "    next_token_logits = model_outputs[0][:, -1, :]\n",
    "    past = model_outputs[1]\n",
    "   \n",
    "    if x == 3:\n",
    "        next_token_to_add = next_token_logits[0].topk(4).indices[1]\n",
    "    else:\n",
    "        next_token_to_add= next_token_logits[0].argmax()\n",
    "    decoded_predictions = []\n",
    "    for tok in next_token_logits[0].topk(10).indices:\n",
    "        decoded_predictions.append(ROMANCE_en_tokenizer.convert_ids_to_tokens(tok.item()).replace('\\u2581', '\\u00a0'))\n",
    "\n",
    "    prediction_list.append(decoded_predictions)\n",
    "\n",
    "    partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "    x+= 1\n",
    "\n",
    "decoded_tokens = [sub.replace('\\u2581', '\\u00a0') for sub in ROMANCE_en_tokenizer.convert_ids_to_tokens(partial_decode[0])]\n",
    "decoded_tokens.remove(\"<pad>\")\n",
    "final = ROMANCE_en_tokenizer.decode(partial_decode[0]).split(\"<pad>\")[1]\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Creo que esta es la tienda equivocada.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits[0].topk(3).indices[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
