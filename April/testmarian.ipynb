{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ROMANCE_model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
    "en_ROMANCE_tokenizer = MarianTokenizer.from_pretrained(en_ROMANCE_model_name)\n",
    "en_ROMANCE = MarianMTModel.from_pretrained(en_ROMANCE_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMANCE_en_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "ROMANCE_en_tokenizer = MarianTokenizer.from_pretrained(ROMANCE_en_model_name)\n",
    "ROMANCE_en = MarianMTModel.from_pretrained(ROMANCE_en_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_generation(english_only, english, start, prefix_only):\n",
    "    if english_only:\n",
    "        #translate english to spanish\n",
    "        engbatch = en_ROMANCE_tokenizer.prepare_translation_batch([english]).to(device)\n",
    "        eng_to_spanish = en_ROMANCE.generate(**engbatch)\n",
    "        machine_translation = en_ROMANCE_tokenizer.decode(eng_to_spanish[0])\n",
    "\n",
    "        #prepare spanish to be translated back to english\n",
    "        tokenizer = ROMANCE_en_tokenizer\n",
    "        model = ROMANCE_en\n",
    "        batchstr = \">>en<<\" + machine_translation.replace(\"<pad> \", '')\n",
    "        tokenized_prefix = tokenizer.convert_tokens_to_ids(en_ROMANCE_tokenizer.tokenize(start))\n",
    "\n",
    "    #prepare english to be translated to spanish\n",
    "    else:\n",
    "        tokenizer = en_ROMANCE_tokenizer\n",
    "        model = en_ROMANCE\n",
    "        batchstr = \">>es<<\" + english\n",
    "        tokenized_prefix = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(start))\n",
    "\n",
    "    prefix = torch.LongTensor(tokenized_prefix).to(device)\n",
    "\n",
    "    batch = tokenizer.prepare_translation_batch([batchstr]).to(device)\n",
    "    original_encoded = model.get_encoder()(**batch)\n",
    "    decoder_start_token = model.config.decoder_start_token_id\n",
    "    partial_decode = torch.LongTensor([decoder_start_token]).to(device).unsqueeze(0)\n",
    "    past = (original_encoded, None)\n",
    "\n",
    "    #machine translation for comparative purposes\n",
    "    translation_tokens = model.generate(**batch)\n",
    "    machine_translation = tokenizer.decode(translation_tokens[0]).split(\"<pad>\")[1]\n",
    "\n",
    "    num_tokens_generated = 0\n",
    "    prediction_list = []\n",
    "    MAX_LENGTH = 100\n",
    "    total = 0\n",
    "\n",
    "    #generate tokens incrementally \n",
    "    while True:\n",
    "        model_inputs = model.prepare_inputs_for_generation(\n",
    "            partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=model.config.use_cache\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model(**model_inputs)\n",
    "\n",
    "        next_token_logits = model_outputs[0][:, -1, :]\n",
    "        past = model_outputs[1]\n",
    "        \n",
    "        #start with designated beginning\n",
    "        if num_tokens_generated < len(prefix):\n",
    "            next_token_to_add = prefix[num_tokens_generated]\n",
    "        elif prefix_only == True:\n",
    "            break\n",
    "        else:\n",
    "            next_token_to_add = next_token_logits[0].argmax()\n",
    "\n",
    "        #calculate score\n",
    "        next_token_logprobs = next_token_logits - next_token_logits.logsumexp(1, True)\n",
    "        token_score = next_token_logprobs[0][next_token_to_add].item()\n",
    "        total += token_score\n",
    "\n",
    "        #append top 10 predictions for each token to list\n",
    "        decoded_predictions = []\n",
    "        for tok in next_token_logits[0].topk(10).indices:\n",
    "            decoded_predictions.append(tokenizer.convert_ids_to_tokens(tok.item()).replace('\\u2581', '\\u00a0'))\n",
    "        \n",
    "        #list of lists of predictions\n",
    "        prediction_list.append(decoded_predictions)\n",
    "\n",
    "        #add new token to tokens so far\n",
    "        partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "        num_tokens_generated += 1\n",
    "\n",
    "        #stop generating at </s>, or when max num tokens exceded\n",
    "        if next_token_to_add.item() == 0 or not (num_tokens_generated < MAX_LENGTH):\n",
    "            break\n",
    "\n",
    "    #list of tokens used to display sentence\n",
    "    decoded_tokens = [sub.replace('\\u2581', '\\u00a0') for sub in tokenizer.convert_ids_to_tokens(partial_decode[0])]\n",
    "    decoded_tokens.remove(\"<pad>\")\n",
    "\n",
    "    final = tokenizer.decode(partial_decode[0]).replace(\"<pad>\", '')\n",
    "    score = round(total/(len(decoded_tokens)), 3)\n",
    "\n",
    "    if english_only:\n",
    "        new_english = final\n",
    "    #back translate spanish into english\n",
    "    else:\n",
    "        batch2 = ROMANCE_en_tokenizer.prepare_translation_batch([\">>en<< \" + final]).to(device)\n",
    "        spanish_to_english = ROMANCE_en.generate(**batch2)\n",
    "        new_english = ROMANCE_en_tokenizer.decode(spanish_to_english[0]).replace(\"<pad>\", '')\n",
    "\n",
    "    return {\"translation\": final,\n",
    "                    \"expected\" : machine_translation,\n",
    "                    \"newEnglish\" : new_english,\n",
    "                    \"tokens\" : decoded_tokens,\n",
    "                    \"predictions\" : prediction_list,\n",
    "                    \"score\" : score\n",
    "                }\n",
    "\n",
    "def rearrange(english, start, first_phrase, auto):\n",
    "    wordlist = [''.join(x for x in par if x not in string.punctuation) for par in english.split(' ')]\n",
    "    first_phrases = set([word.capitalize() for word in wordlist])\n",
    "\n",
    "    #get most likely sentence or prefix and its score, given the word to move towards front\n",
    "    def get_alt(start, prefix_only):\n",
    "        if start[0] in wordlist:\n",
    "            pos = wordlist.index(start.lstrip())\n",
    "        #if subword token is selected\n",
    "        else:\n",
    "            res = [i for i in wordlist if start[0].lstrip() in i]\n",
    "            pos = wordlist.index(res[0])\n",
    "\n",
    "        #word before selected word\n",
    "        first_phrases.add(wordlist[pos - 1].capitalize())\n",
    "        #2 words before selected word\n",
    "        first_phrases.add(' '.join(wordlist[pos-2: pos]).capitalize())\n",
    "        first_phrases.add('The')\n",
    "        if first_phrase != '':\n",
    "            prefixes = [first_phrase + ' ' + word.lower() + ' ' + start.lstrip() for word in first_phrases]\n",
    "        else:\n",
    "            prefixes = [first_phrase + word + ' ' + start.lstrip() for word in first_phrases]\n",
    "        prefixes.append(start.lstrip().capitalize())\n",
    "\n",
    "        results = []\n",
    "        scores = []\n",
    "\n",
    "        #score each possible prefix/sentence\n",
    "        for prefix in prefixes:\n",
    "            data = incremental_generation(english_only=True, english=english, start=prefix, prefix_only=prefix_only)\n",
    "            results.append(data[\"translation\"])\n",
    "            scores.append(data[\"score\"])\n",
    "        #select most likely sentence or prefix\n",
    "        ind = scores.index(max(scores))\n",
    "        winner = results[ind]\n",
    "        winnerscore = scores[ind]\n",
    "        return (winnerscore, winner)\n",
    "\n",
    "    alternatives = []\n",
    "    winner = ''\n",
    "\n",
    "    #generate a list of alternatives\n",
    "    if auto:\n",
    "        #skip first 3 words bc they all return the default sentence\n",
    "        for word in wordlist[3:]:\n",
    "            alt = get_alt(word, prefix_only=False)\n",
    "            #avoid duplicate sentences\n",
    "            if alt not in alternatives:\n",
    "                alternatives.append(alt)\n",
    "            sorted_scores = sorted(((score, result) for score, result in alternatives), reverse=True)\n",
    "        alternatives = [pair[1] for pair in sorted_scores]\n",
    "        return {\"alternatives\" : alternatives}\n",
    "\n",
    "    else:\n",
    "        #get most likely prefix\n",
    "        winner = get_alt(start, prefix_only=False)[1]\n",
    "        #get full sentence given prefix\n",
    "        return incremental_generation(english_only=True, english=english, start=winner, prefix_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellowstone Yellowstone PROPN NNP compound Xxxxx True False\n",
      "National National PROPN NNP compound Xxxxx True False\n",
      "Park Park PROPN NNP nsubjpass Xxxx True False\n",
      "was be AUX VBD auxpass xxx True True\n",
      "established establish VERB VBN ROOT xxxx True False\n",
      "by by ADP IN agent xx True True\n",
      "the the DET DT det xxx True True\n",
      "US US PROPN NNP compound XX True True\n",
      "government government NOUN NN pobj xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "1972 1972 NUM CD pobj dddd False False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Yellowstone National Park was established by the US government in 1972.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellowstone National Park - Park - nsubjpass - established\n",
      "the US government - government - pobj - by\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text,'-', chunk.root.text,'-', chunk.root.dep_, '-',\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alternatives': [' The Yellowstone was established by the United States government in 1972.',\n",
       "  ' In 1972, Yellowstone was established by the United States government.',\n",
       "  ' The government of the United States established Yellowstone in 1972.',\n",
       "  ' By the United States government in 1972, Yellowstone was established.',\n",
       "  ' The US government established Yellowstone in 1972.']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = rearrange(\"Yellowstone was established by the US government in 1972.\", '', '', auto=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The cat is given a piece of chicken by George.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(\"George gave the cat a piece of chicken.\", \"cat\", '', False)['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearrange(\"George gave the cat a piece of chicken.\", \"cat\", '', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' The Yellowstone was established by the United States government in 1972.',\n",
       " ' In 1972, Yellowstone was established by the United States government.',\n",
       " ' The government of the United States established Yellowstone in 1972.',\n",
       " ' By the United States government in 1972, Yellowstone was established.',\n",
       " ' The US government established Yellowstone in 1972.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['alternatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' A piece of chicken was given to the cat by George.',\n",
       " ' The cat is given a piece of chicken by George.',\n",
       " ' Of course, George gave the cat a piece of chicken.',\n",
       " ' A chicken piece is given to the cat by George.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(\"George gave the cat a piece of chicken.\", \"\", '', True)['alternatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Yellowstone was established by the US government in 1972.\"\n",
    "first_select = \"Yellowstone\"\n",
    "second_select = \"1972\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yellowstone'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = rearrange(sentence, first_select, '', auto=False)\n",
    "new_prefix = data['translation']\n",
    "new_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yellowstone in 1972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Yellowstone in 1972 was established by the United States government.'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = rearrange(sentence, second_select, new_prefix, auto=False)\n",
    "print(data2['translation'])\n",
    "final = incremental_generation(english_only=True, english=sentence, start=data2['translation'], prefix_only=False)\n",
    "final['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prefix(machine_translation, prefix):\n",
    "    tokenizer = ROMANCE_en_tokenizer\n",
    "    model = ROMANCE_en\n",
    "    tokenized_prefix = tokenizer.convert_tokens_to_ids(en_ROMANCE_tokenizer.tokenize(prefix.strip()))\n",
    "    prefix = torch.LongTensor(tokenized_prefix).to(device)\n",
    "\n",
    "    batch = tokenizer.prepare_translation_batch([machine_translation.replace(\"<pad> \", '')]).to(device)\n",
    "    english_encoded = model.get_encoder()(**batch)\n",
    "    decoder_start_token = model.config.decoder_start_token_id\n",
    "    # pylint: disable=E1101\n",
    "    partial_decode = torch.LongTensor([decoder_start_token]).to(device).unsqueeze(0)\n",
    "    past = (english_encoded, None)\n",
    "    # pylint: enable=E1101\n",
    "    num_tokens_generated = 0\n",
    "    total = 0\n",
    "    MAX_LENGTH = 100\n",
    "    \n",
    "    #stop when </s> token generated, or max num tokens exceded (just in case)\n",
    "    while True:\n",
    "        model_inputs = model.prepare_inputs_for_generation(\n",
    "        partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=model.config.use_cache\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model(**model_inputs)\n",
    "        next_token_logits = model_outputs[0][:, -1, :]\n",
    "        past = model_outputs[1]\n",
    "        #start with user inputted beginning\n",
    "        if num_tokens_generated < len(prefix):\n",
    "            next_token_to_add = prefix[num_tokens_generated]\n",
    "        else:\n",
    "            next_token_to_add = next_token_logits[0].argmax()\n",
    "        next_token_logprobs = next_token_logits - next_token_logits.logsumexp(1, True)\n",
    "        token_score = next_token_logprobs[0][next_token_to_add].item()\n",
    "        total += token_score\n",
    "\n",
    "        #add new token to tokens so far\n",
    "        partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "        num_tokens_generated+= 1\n",
    "\n",
    "        if next_token_to_add.item() == 0 or not (num_tokens_generated < MAX_LENGTH):\n",
    "            break\n",
    "\n",
    "    #list of tokens used to display sentence\n",
    "    decoded_tokens = [sub.replace('\\u2581', '\\u00a0') for sub in tokenizer.convert_ids_to_tokens(partial_decode[0])]\n",
    "    decoded_tokens.remove(\"<pad>\")\n",
    "\n",
    "    final = tokenizer.decode(partial_decode[0]).replace(\"<pad>\", '')\n",
    "    score = round(total/(len(decoded_tokens)), 3)\n",
    "\n",
    "    return (score, final.lstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " '1972',\n",
       " 'By',\n",
       " 'Established',\n",
       " 'Government',\n",
       " 'In',\n",
       " 'National',\n",
       " 'Park',\n",
       " 'The',\n",
       " 'Us',\n",
       " 'Was',\n",
       " 'Yellowstone'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"Yellowstone was established by the US government in 1972.\"\n",
    "wordlist = [''.join(x for x in par if x not in punctuation) for par in input_str.split(' ')]\n",
    "first_phrases = set([word.capitalize() for word in wordlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = \">>es<<\" + input_str\n",
    "engbatch = en_ROMANCE_tokenizer.prepare_translation_batch([english]).to(device)\n",
    "eng_to_spanish = en_ROMANCE.generate(**engbatch).to(device)\n",
    "machine_translation = en_ROMANCE_tokenizer.decode(eng_to_spanish[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yellowstone was established by the U.S. government in 1972.'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = score_prefix(machine_translation, wordlist[0])[1]\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yellowstone was established by the US government in 1972. Yellowstone was established by the U.S. government in 1972.'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str + ' ' + expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scores = []\n",
    "for word in wordlist:\n",
    "    pos = wordlist.index(word.lstrip())\n",
    "    first_phrases.add(' '.join(wordlist[pos-2: pos]).capitalize())\n",
    "    for phrase in first_phrases:\n",
    "        res = score_prefix(machine_translation, phrase)\n",
    "        simple = str(res[1].lower()).replace('the ', '')\n",
    "        if simple not in expected.lower() and expected.lower() not in simple:\n",
    "            sent_scores.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >sentence</th>        <th class=\"col_heading level0 col1\" >probability</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row0_col0\" class=\"data row0 col0\" >Yellowstone was established by the U.S. government in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row0_col1\" class=\"data row0 col1\" >-0.160000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row1_col0\" class=\"data row1 col0\" >The Yellowstone was established by the U.S. government in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row1_col1\" class=\"data row1 col1\" >-0.510000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row2_col0\" class=\"data row2 col0\" >In 1972, Yellowstone was established by the U.S. government.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row2_col1\" class=\"data row2 col1\" >-0.638000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row3_col0\" class=\"data row3 col0\" >Established by the U.S. government in 1972, Yellowstone was established in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row3_col1\" class=\"data row3 col1\" >-1.221000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row4_col0\" class=\"data row4 col0\" >By the US government, Yellowstone was established in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row4_col1\" class=\"data row4 col1\" >-1.229000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row5_col0\" class=\"data row5 col0\" >Was established by the US government in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row5_col1\" class=\"data row5 col1\" >-1.353000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row6_col0\" class=\"data row6 col0\" >Usher was established by the U.S. government in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row6_col1\" class=\"data row6 col1\" >-1.459000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row7_col0\" class=\"data row7 col0\" >Government of the United States established Yellowstone in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row7_col1\" class=\"data row7 col1\" >-1.504000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row8_col0\" class=\"data row8 col0\" >The ushers of Yellowstone was established by the US government in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row8_col1\" class=\"data row8 col1\" >-1.539000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row9_col0\" class=\"data row9 col0\" >1972 was the first year of the American government to establish Yellowstone.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row9_col1\" class=\"data row9 col1\" >-1.945000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row10_col0\" class=\"data row10 col0\" >Government in 1972 established Yellowstone.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row10_col1\" class=\"data row10 col1\" >-2.510000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row11_col0\" class=\"data row11 col0\" >Us government established Yellowstone in 1972.</td>\n",
       "                        <td id=\"T_cd9a5afe_c253_11ea_aeaa_b42e993e0d63row11_col1\" class=\"data row11 col1\" >-2.927000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3f49705990>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_scores = sorted(((score, result) for score, result in set(sent_scores)), reverse=True)\n",
    "results = pd.DataFrame({'sentence': [pair[1] for pair in sorted_scores],\n",
    "              'probability': [pair[0] for pair in sorted_scores]}).style.hide_index()\n",
    "# df = df.style.set_properties(**{'text-align': 'left'})\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.colheader_justify\",\"left\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >sentence</th>        <th class=\"col_heading level0 col1\" >probability</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row0_col0\" class=\"data row0 col0\" >In my opinion, all cats are great.</td>\n",
       "                        <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row0_col1\" class=\"data row0 col1\" >-0.313000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row1_col0\" class=\"data row1 col0\" >All cats are great, in my opinion.</td>\n",
       "                        <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row1_col1\" class=\"data row1 col1\" >-0.765000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row2_col0\" class=\"data row2 col0\" >My guess is, all cats are great.</td>\n",
       "                        <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row2_col1\" class=\"data row2 col1\" >-0.770000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row3_col0\" class=\"data row3 col0\" >Great. In my opinion, all cats are great.</td>\n",
       "                        <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row3_col1\" class=\"data row3 col1\" >-1.229000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row4_col0\" class=\"data row4 col0\" >Are you sure? In my opinion, all cats are great.</td>\n",
       "                        <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row4_col1\" class=\"data row4 col1\" >-1.230000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row5_col0\" class=\"data row5 col0\" >Cats are all great, in my opinion.</td>\n",
       "                        <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row5_col1\" class=\"data row5 col1\" >-1.349000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row6_col0\" class=\"data row6 col0\" >Opinion is, all cats are great.</td>\n",
       "                        <td id=\"T_fa4a34c0_c082_11ea_aa68_b42e993e0d63row6_col1\" class=\"data row6 col1\" >-1.718000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f69b924f4d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_scores = sorted(((score, result) for score, result in sent_scores), reverse=True)\n",
    "results = pd.DataFrame({'sentence': [pair[1] for pair in sorted_scores],\n",
    "              'probability': [pair[0] for pair in sorted_scores]}).style.hide_index()\n",
    "# df = df.style.set_properties(**{'text-align': 'left'})\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.colheader_justify\",\"left\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.5\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.806, 'All cats')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_prefix(\"In my opinion, all cats are great.\", \"All cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['599', 'All', 'All of the', 'Been', 'Before', 'Cats', 'Charasmatic', 'Confirming', 'Day', 'Easter', 'Easter Island has long been', 'Established', 'Famous', 'Fundamental', 'Go', 'Has', 'Held', 'Individuals', 'Is', 'Island', 'Long', 'Longmire', 'Longmire Day is', 'National', 'Night', 'Of', 'Once', 'Once they', 'Our', 'Our fundamental', 'Park', 'Species', 'The', 'They', 'Those', 'Those charasmatic', 'Values', 'Was', 'Yellowstone', 'Yellowstone National Park was']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['All of the cats are great, in my opinion.',\n",
       " 'The night was dark and stormy.',\n",
       " \"Once they go, they won't be back.\",\n",
       " 'Easter Island has long been famous for the enormous stone statues erected by its prehistoric settlers.',\n",
       " \"Yellowstone National Park was established by the US government in 1972 as the world's first legislated effort at nature conservation.\",\n",
       " 'Before confirming cancer, a positive PSA text has to be followed up with a biopsy or other procedures.',\n",
       " \"Our fundamental values haven't changed, said University of Michigan President Mary Sue Coleman in a statement on the university's Web site.\",\n",
       " 'Those charasmatic species whose specieswide genetic diversity is very low are at the highest risk.',\n",
       " \"Longmire Day is held in the small town of Buffalo, Wyoming to celebrate the success of Johnson's novels.\",\n",
       " '599 individuals lived there at the 2006 census.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "    ('In my opinion, all of the cats are great.', 'All of the cats', 'cats'),\n",
    "    ('It was a dark and stormy night', 'The night', 'night'),\n",
    "    ('They won\\'t be back once they go.', 'Once they go', 'go'),\n",
    "    ('''Easter Island, the most remote place on Earth in terms of distance from other inhabited places, \n",
    "         has long been famous for the enormous stone statues erected by its prehistoric settlers.''', 'Easter Island has long been famous', 'famous'),\n",
    "    ('In 1972, the United States government established Yellowstone National Park as the world\\'s first legislated effort at nature conservation',\n",
    "        'Yellowstone National Park was established', 'established'),\n",
    "    ('A positive PSA test has to be followed up with a biopsy or other procedures before cancer can be confirmed.', 'Before confirming', 'confirming'),\n",
    "    ('University of Michigan President Mary Sue Coleman said in a statement on the university\\'s Web site, \"Our fundamental values haven\\'t changed.',\n",
    "        'Our fundamental values', 'values'),\n",
    "    ('At highest risk are those charasmatic species whose specieswide genetic diversity is very low.', 'Those charasmatic species', 'species'),\n",
    "     ('The success of Johnson\\'s novels is celebrated in an annual festival, called Longmire Day, held in the small town of Buffalo, Wyoming', 'Longmire Day is held', 'held'),\n",
    "    ('At the 2006 census, its population was 599', '599 individuals', 'individuals')\n",
    "]\n",
    "first_phrases = []\n",
    "for src, tgt, word in examples:\n",
    "    #tgt_words = tgt#.split()\n",
    "    #idx = tgt_words.index(word)\n",
    "    #words_before_tgt_word = tgt_words[:idx]\n",
    "    for w in tgt.rstrip('?.!,').split():\n",
    "        first_phrases.append(w.capitalize())\n",
    "    first_phrases.append(tgt[0: tgt.index(word)].strip())\n",
    "print(sorted(set(first_phrases)))\n",
    "\n",
    "paraphrases = [\"All of the cats are great, in my opinion.\",\n",
    "               \"The night was dark and stormy.\",\n",
    "               \"Once they go, they won't be back.\",\n",
    "               \"Easter Island has long been famous for the enormous stone statues erected by its prehistoric settlers.\",\n",
    "               \"Yellowstone National Park was established by the US government in 1972 as the world\\'s first legislated effort at nature conservation.\",\n",
    "               \"Before confirming cancer, a positive PSA text has to be followed up with a biopsy or other procedures.\",\n",
    "               \"Our fundamental values haven't changed, said University of Michigan President Mary Sue Coleman in a statement on the university's Web site.\",\n",
    "               \"Those charasmatic species whose specieswide genetic diversity is very low are at the highest risk.\",\n",
    "               \"Longmire Day is held in the small town of Buffalo, Wyoming to celebrate the success of Johnson's novels.\",\n",
    "               \"599 individuals lived there at the 2006 census.\"\n",
    "              ]\n",
    "paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7dcbf0a123ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_phrases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprefixes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# compute rank of actual prefix in sorted scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msorted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-7dcbf0a123ca>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_phrases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprefixes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# compute rank of actual prefix in sorted scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msorted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bd15e8ca1b37>\u001b[0m in \u001b[0;36mscore_prefix\u001b[0;34m(src, prefix)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menglish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\">>es<<\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mengbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_ROMANCE_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_translation_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menglish\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0meng_to_spanish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_ROMANCE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mengbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmachine_translation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_ROMANCE_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_to_spanish\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m                 \u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             )\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_generate_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, early_stopping, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, num_return_sequences, length_penalty, num_beams, vocab_size, encoder_outputs, attention_mask, use_cache, model_specific_kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             )\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size * num_beams, cur_len, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (batch_size * num_beams, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/transformers/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, decoder_cached_states, labels, use_cache, output_attentions, output_hidden_states, **unused)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m         )\n\u001b[1;32m   1005\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/transformers/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, encoder_outputs, decoder_attention_mask, decoder_cached_states, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m         )\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/transformers/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, encoder_hidden_states, encoder_padding_mask, decoder_padding_mask, decoder_causal_mask, decoder_cached_states, use_cache, output_attentions, output_hidden_states, **unused)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mlayer_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_causal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             )\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/transformers/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_hidden_states, encoder_attn_mask, layer_state, causal_mask, decoder_padding_mask, output_attentions)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Cross attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 153\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \"\"\"\n\u001b[1;32m   1955\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 1956\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   1957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "for src, tgt, word in examples:\n",
    "    prefixes = [phrase +  ' ' + word for phrase in first_phrases]\n",
    "    scores = [score_prefix(src, prefix) for prefix in prefixes]\n",
    "    # compute rank of actual prefix in sorted scores\n",
    "    sorted_scores = sorted(((score, result) for score, result in scores), reverse=True)\n",
    "    rank = [y[1] for y in sorted_scores].index(tgt) + 1\n",
    "    ranks.append((tgt, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'original': [ex[0] for ex in examples],\n",
    "              'expected paraphrase': paraphrases,\n",
    "              'expected prefix': [ex[1] for ex in examples],\n",
    "              'word': [ex[2] for ex in examples],\n",
    "              'rank of expected prefix': [el[1] for el in ranks]}).style.hide_index()\n",
    "# df = df.style.set_properties(**{'text-align': 'left'})\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.colheader_justify\",\"left\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'scores': [prefix[0] for prefix in scores], 'sentences': [prefix[1] for prefix in scores]}).sort_values('scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100])\n",
      "{'translation': ' La noche fue oscura y tempestuosa.', 'expected': ' Era una noche oscura y tempestuosa.', 'tokens': ['\\xa0La', '\\xa0noche', '\\xa0fue', '\\xa0os', 'cura', '\\xa0y', '\\xa0tem', 'pes', 'tu', 'osa', '.', '</s>'], 'predictions': [['\\xa0Era', '\\xa0Fue', '\\xa0Ha', '\\xa0Es', '\\xa0Estaba', '\\xa0Había', '\\xa0Una', '\\xa0La', '\\xa0-', '\\xa0No'], ['\\xa0noche', '\\xa0tarde', '\\xa0oscuridad', '\\xa0vela', '\\xa0verdad', '\\xa0misma', '\\xa0no', '\\xa0ciudad', '\\xa0cosa', '\\xa0última'], ['\\xa0fue', '\\xa0era', '\\xa0estaba', '\\xa0estuvo', '\\xa0es', '\\xa0había', '\\xa0se', '\\xa0de', '\\xa0no', '\\xa0que'], ['\\xa0os', '\\xa0som', '\\xa0tem', '\\xa0muy', '\\xa0de', '\\xa0oscuro', '\\xa0tan', '\\xa0tor', '\\xa0negra', '\\xa0o'], ['cura', 'cure', 'cur', 'cu', 'ca', 'curi', 'erta', 'jera', 'curr', 'cor'], ['\\xa0y', ',', '.', '\\xa0e', '...', '\\xa0de', '<pad>', '\\xa0con', '\\xa0en', '\\xa0pero'], ['\\xa0tem', '\\xa0tormenta', '\\xa0tor', '\\xa0tur', '\\xa0os', '\\xa0de', '\\xa0tort', '\\xa0o', '\\xa0muy', '\\xa0torre'], ['pes', 'p', 'pra', 'pla', 'pul', 'pl', 'b', 'ps', 'bla', 'blo'], ['tu', 'ta', 'tada', 'table', 'tiva', 'tante', 'tá', 'un', 'tó', 'tée'], ['osa', 'oso', 'osas', 'ada', 'osamente', 'ou', 'enta', 'a', 'euse', 'ísima'], ['.', '</s>', ',', '...', '<pad>', '!', '\\xa0', ';', ':', '\\xa0y'], ['</s>', '\\xa0¿', '<pad>', '\\xa0-', '\\xa0No', '.', '\\xa0¡', '\\xa0El', '\\xa0Y', '\\xa0']], ' - The night was dark and stormy.': ' - The night was dark and stormy.'}\n"
     ]
    }
   ],
   "source": [
    "def incremental_translation(batchstr, starting_word):\n",
    "    prefix = torch.LongTensor(starting_word)\n",
    "    print(prefix)\n",
    "    batch = tokenizer.prepare_translation_batch([batchstr])\n",
    "    translation_tokens = model.generate(**batch)\n",
    "    machine_translation = tokenizer.decode(translation_tokens[0]).split(\"<pad>\")[1]\n",
    "    original_encoded = model.get_encoder()(**batch)\n",
    "    decoder_start_token = model.config.decoder_start_token_id\n",
    "\n",
    "    partial_decode = torch.LongTensor([decoder_start_token]).unsqueeze(0)\n",
    "    past = (original_encoded, None)\n",
    "    next_token_to_add = torch.tensor(1)\n",
    "    num_tokens_generated = 0\n",
    "\n",
    "    prediction_list = []\n",
    "    MAX_LENGTH = 100\n",
    "\n",
    "    while True:\n",
    "        model_inputs = model.prepare_inputs_for_generation(\n",
    "        partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=model.config.use_cache\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model(**model_inputs)\n",
    "\n",
    "        next_token_logits = model_outputs[0][:, -1, :]\n",
    "        past = model_outputs[1]\n",
    "\n",
    "        #start with designated beginning\n",
    "        if num_tokens_generated < len(prefix):\n",
    "            next_token_to_add = prefix[num_tokens_generated]\n",
    "        else:\n",
    "            next_token_to_add = next_token_logits[0].argmax()\n",
    "\n",
    "        #append top 10 predictions for each token to list\n",
    "        decoded_predictions = []\n",
    "        for tok in next_token_logits[0].topk(10).indices:\n",
    "            decoded_predictions.append(tokenizer.convert_ids_to_tokens(tok.item()).replace('\\u2581', '\\u00a0'))\n",
    "        #list of lists of predictions\n",
    "        prediction_list.append(decoded_predictions)\n",
    "\n",
    "        #add new token to tokens so far\n",
    "        partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "        num_tokens_generated+= 1\n",
    "\n",
    "        if next_token_to_add.item() == 0 or not (num_tokens_generated < MAX_LENGTH):\n",
    "            break\n",
    "\n",
    "    #list of tokens used to display sentence\n",
    "    decoded_tokens = [sub.replace('\\u2581', '\\u00a0') for sub in tokenizer.convert_ids_to_tokens(partial_decode[0])]\n",
    "    decoded_tokens.remove(\"<pad>\")\n",
    "\n",
    "    final = tokenizer.decode(partial_decode[0]).replace(\"<pad>\", '')\n",
    "   \n",
    "    return {\"translation\": final,\n",
    "                \"expected\" : machine_translation,\n",
    "                \"tokens\" : decoded_tokens,\n",
    "                \"predictions\" : prediction_list\n",
    "            }\n",
    "\n",
    "skip = \"false\"\n",
    "copy_input = \"false\"\n",
    "english = \"It was a dark and stormy night.\"\n",
    "\n",
    "if copy_input == \"true\":\n",
    "    start = english\n",
    "else:\n",
    "    start = \"La\"\n",
    "\n",
    "#english only- spanish behind the scenes\n",
    "if skip == \"true\":\n",
    "    engbatch = en_ROMANCE_tokenizer.prepare_translation_batch([english])\n",
    "    eng_to_spanish = en_ROMANCE.generate(**engbatch)\n",
    "    mid_machine_translation = en_ROMANCE_tokenizer.decode(eng_to_spanish[0])\n",
    "    tokenizer = ROMANCE_en_tokenizer\n",
    "    model = ROMANCE_en\n",
    "    batchstr = \">>en<<\" + mid_machine_translation.replace(\"<pad> \", '')\n",
    "    starting_word = tokenizer.convert_tokens_to_ids(en_ROMANCE_tokenizer.tokenize(start))\n",
    "    data = incremental_translation(batchstr, starting_word)\n",
    "    data.update( { \"new_english\" : data[\"translation\"] })\n",
    "    print(data)\n",
    "    \n",
    "#show spanish\n",
    "else:\n",
    "    tokenizer = en_ROMANCE_tokenizer\n",
    "    model = en_ROMANCE\n",
    "    batchstr = \">>es<<\" + english\n",
    "    starting_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(start))\n",
    "    data = incremental_translation(batchstr, starting_word)\n",
    "    batch2 = ROMANCE_en_tokenizer.prepare_translation_batch([\">>en<< \" + final])\n",
    "    spanish_to_english = ROMANCE_en.generate(**batch2)\n",
    "    new_english = ROMANCE_en_tokenizer.decode(spanish_to_english[0]).replace(\"<pad>\", '')\n",
    "    data.update({new_english : new_english})\n",
    "    print(data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(' ' + 'All cats are great, info my opinion').index(' ' + 'in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a\\nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'a\\nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'\\bgreat\\b', 'All of the cats are great, opinion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = \">>es<<In my opinion, all cats are great.\"\n",
    "engbatch = en_ROMANCE_tokenizer.prepare_translation_batch([english])\n",
    "eng_to_spanish = en_ROMANCE.generate(**engbatch)\n",
    "machine_translation = en_ROMANCE_tokenizer.decode(eng_to_spanish[0])\n",
    "\n",
    "start = \"cats\"\n",
    "\n",
    "tokenizer = ROMANCE_en_tokenizer\n",
    "model = ROMANCE_en\n",
    "\n",
    "first_words = [\"The\", \"All\", \"\"]\n",
    "wordlist = english.split(' ')\n",
    "for word in wordlist:\n",
    "    first_words.append(word.capitalize())\n",
    "first_words\n",
    "    \n",
    "results = []\n",
    "scores = []\n",
    "MAX_LENGTH = 100\n",
    "    \n",
    "for word in first_words:\n",
    "    join_prefix_str = word + \" \" + start\n",
    "    tokenized_prefix = tokenizer.convert_tokens_to_ids(en_ROMANCE_tokenizer.tokenize(join_prefix_str.strip()))\n",
    "    prefix = torch.LongTensor(tokenized_prefix)\n",
    "#     tokenized_prefix = tokenizer.convert_tokens_to_ids(en_ROMANCE_tokenizer.tokenize(start))\n",
    "#     prefix = torch.LongTensor(tokenized_prefix)\n",
    "\n",
    "    batch = tokenizer.prepare_translation_batch([machine_translation.replace(\"<pad> \", '')])\n",
    "    english_encoded = model.get_encoder()(**batch)\n",
    "    decoder_start_token = model.config.decoder_start_token_id\n",
    "    # pylint: disable=E1101\n",
    "    partial_decode = torch.LongTensor([decoder_start_token]).unsqueeze(0)\n",
    "    past = (english_encoded, None)\n",
    "    # pylint: enable=E1101\n",
    "\n",
    "    num_tokens_generated = 0\n",
    "    prediction_list = []\n",
    "    total = 0\n",
    "    #stop when </s> token generated, or max num tokens exceded (just in case)\n",
    "    while True:\n",
    "        model_inputs = model.prepare_inputs_for_generation(\n",
    "        partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=model.config.use_cache\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model(**model_inputs)\n",
    "\n",
    "        next_token_logits = model_outputs[0][:, -1, :]\n",
    "        past = model_outputs[1]\n",
    "\n",
    "        #start with user inputted beginning\n",
    "        if num_tokens_generated < len(prefix):\n",
    "            next_token_to_add = prefix[num_tokens_generated]\n",
    "        else:\n",
    "            break\n",
    "#             next_token_to_add = next_token_logits[0].argmax()\n",
    "\n",
    "        next_token_logprobs = next_token_logits - next_token_logits.logsumexp(1, True)\n",
    "        score = next_token_logprobs[0][next_token_to_add].item()\n",
    "\n",
    "        total += score\n",
    "\n",
    "        #add new token to tokens so far\n",
    "        partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "        num_tokens_generated+= 1\n",
    "\n",
    "        if next_token_to_add.item() == 0 or not (num_tokens_generated < MAX_LENGTH):\n",
    "            break\n",
    "\n",
    "    #list of tokens used to display sentence\n",
    "    decoded_tokens = [sub.replace('\\u2581', '\\u00a0') for sub in tokenizer.convert_ids_to_tokens(partial_decode[0])]\n",
    "    decoded_tokens.remove(\"<pad>\")\n",
    "\n",
    "    final = tokenizer.decode(partial_decode[0]).replace(\"<pad>\", '')\n",
    "    score = round(total/(len(decoded_tokens)), 3)\n",
    "    results.append(final)\n",
    "    scores.append(score)\n",
    "\n",
    "    print(\"\\n\" + final)\n",
    "    print(total)\n",
    "\n",
    "ind = scores.index(max(scores))\n",
    "winner = results[ind]\n",
    "print(\"\\nMost likely: \", winner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
