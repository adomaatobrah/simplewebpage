{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import spacy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ROMANCE_model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
    "en_ROMANCE_tokenizer = MarianTokenizer.from_pretrained(en_ROMANCE_model_name)\n",
    "en_ROMANCE = MarianMTModel.from_pretrained(en_ROMANCE_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMANCE_en_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "ROMANCE_en_tokenizer = MarianTokenizer.from_pretrained(ROMANCE_en_model_name)\n",
    "ROMANCE_en = MarianMTModel.from_pretrained(ROMANCE_en_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monkey_patch(model, new_postproc_fn):\n",
    "    cls = model.__class__\n",
    "    print(cls)\n",
    "    func_name = \"postprocess_next_token_scores\"\n",
    "    orig_name = \"_orig_\" + func_name\n",
    "    if not hasattr(cls, orig_name):\n",
    "        print(str(cls) + \" doesn't have attribute \" + orig_name)\n",
    "        setattr(cls, orig_name, getattr(cls, func_name))\n",
    "    else:\n",
    "        print(str(cls) + \" has attribute \" + orig_name)\n",
    "    setattr(cls, func_name, new_postproc_fn)\n",
    "    print(str(cls) + '\\n' + func_name + '\\n' + str(new_postproc_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_marian.MarianMTModel'>\n",
      "<class 'transformers.modeling_marian.MarianMTModel'> has attribute _orig_postprocess_next_token_scores\n",
      "<class 'transformers.modeling_marian.MarianMTModel'>\n",
      "postprocess_next_token_scores\n",
      "<function postprocess_next_token_scores at 0x7fee08f0d3b0>\n"
     ]
    }
   ],
   "source": [
    "def postprocess_next_token_scores(self, scores, input_ids, *a, **kw):\n",
    "    if not original_postprocess:\n",
    "        batch_size, vocab_size = scores.shape\n",
    "        cur_len = input_ids.shape[1]\n",
    "        for hypothesis_idx in range(batch_size):\n",
    "            cur_hypothesis = input_ids[hypothesis_idx]\n",
    "\n",
    "        if 0 < cur_len <= len(selected_tokens):\n",
    "            force_token_id = selected_tokens[cur_len-1]\n",
    "            self._force_token_ids_generation(scores, token_ids=[force_token_id])\n",
    "#         print(cur_hypothesis)\n",
    "#         print(ROMANCE_en_tokenizer.decode(cur_hypothesis))\n",
    "#         if cur_len == 1:\n",
    "#             force_token_id = selected_token\n",
    "#             self._force_token_ids_generation(scores, token_ids=[force_token_id])\n",
    "\n",
    "    return self._orig_postprocess_next_token_scores(scores, input_ids, *a, **kw)\n",
    "\n",
    "monkey_patch(en_ROMANCE, postprocess_next_token_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prefix(machine_translation, prefix):\n",
    "    tokenizer = ROMANCE_en_tokenizer\n",
    "    model = ROMANCE_en\n",
    "    tokenized_prefix = tokenizer.convert_tokens_to_ids(en_ROMANCE_tokenizer.tokenize(prefix.strip()))\n",
    "    prefix = torch.LongTensor(tokenized_prefix).to(device)\n",
    "\n",
    "    batch = tokenizer.prepare_translation_batch([machine_translation.replace(\"<pad> \", '')]).to(device)\n",
    "    english_encoded = model.get_encoder()(**batch)\n",
    "    decoder_start_token = model.config.decoder_start_token_id\n",
    "    # pylint: disable=E1101\n",
    "    partial_decode = torch.LongTensor([decoder_start_token]).to(device).unsqueeze(0)\n",
    "    past = (english_encoded, None)\n",
    "    # pylint: enable=E1101\n",
    "    num_tokens_generated = 0\n",
    "    total = 0\n",
    "    MAX_LENGTH = 100\n",
    "    \n",
    "    #stop when </s> token generated, or max num tokens exceded (just in case)\n",
    "    while True:\n",
    "        model_inputs = model.prepare_inputs_for_generation(\n",
    "        partial_decode, past=past, attention_mask=batch['attention_mask'], use_cache=model.config.use_cache\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model(**model_inputs)\n",
    "        next_token_logits = model_outputs[0][:, -1, :]\n",
    "        past = model_outputs[1]\n",
    "        #start with user inputted beginning\n",
    "        if num_tokens_generated < len(prefix):\n",
    "            next_token_to_add = prefix[num_tokens_generated]\n",
    "        else:\n",
    "            next_token_to_add = next_token_logits[0].argmax()\n",
    "        next_token_logprobs = next_token_logits - next_token_logits.logsumexp(1, True)\n",
    "        token_score = next_token_logprobs[0][next_token_to_add].item()\n",
    "#         print(token_score)\n",
    "        total += token_score\n",
    "\n",
    "        #add new token to tokens so far\n",
    "        partial_decode = torch.cat((partial_decode, next_token_to_add.unsqueeze(0).unsqueeze(0)), -1)\n",
    "        num_tokens_generated+= 1\n",
    "\n",
    "        if next_token_to_add.item() == 0 or not (num_tokens_generated < MAX_LENGTH):\n",
    "            break\n",
    "\n",
    "    #list of tokens used to display sentence\n",
    "    decoded_tokens = [sub.replace('\\u2581', '\\u00a0') for sub in tokenizer.convert_ids_to_tokens(partial_decode[0])]\n",
    "    decoded_tokens.remove(\"<pad>\")\n",
    "\n",
    "    final = tokenizer.decode(partial_decode[0]).replace(\"<pad>\", '')\n",
    "    score = round(total/(len(decoded_tokens)), 3)\n",
    "\n",
    "    return (score, final.lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(tokenizer, model, text, num_outputs):   \n",
    "    \"\"\"Use beam search to get a reasonable translation of 'text'\"\"\"\n",
    "    # Tokenize the source text\n",
    "    tokenizer.current_spm = tokenizer.spm_source ### HACK!\n",
    "    batch = tokenizer.prepare_translation_batch([text]).to(model.device)\n",
    "    \n",
    "    # Run model\n",
    "    num_beams = num_outputs\n",
    "    translated = model.generate(**batch, num_beams=num_beams, num_return_sequences=num_outputs, max_length=40, no_repeat_ngram_size=5)\n",
    "    \n",
    "    # Untokenize the output text.\n",
    "    tokenizer.current_spm = tokenizer.spm_target\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_postprocess = True;\n",
    "# input_str = \"Yellowstone National Park was established by the US government in 1972 as the world's first legislated effort at nature conservation.\"\n",
    "# english = \">>es<<\" + input_str\n",
    "# engbatch = en_ROMANCE_tokenizer.prepare_translation_batch([english]).to(device)\n",
    "# eng_to_spanish = en_ROMANCE.generate(**engbatch).to(device)\n",
    "# machine_translation = en_ROMANCE_tokenizer.decode(eng_to_spanish[0]).replace(\"<pad> \", '')\n",
    "\n",
    "# results = []\n",
    "# for word in input_str.split(' ')[3:]:\n",
    "#     selection = word\n",
    "#     ROMANCE_en_tokenizer.current_spm = ROMANCE_en_tokenizer.spm_target\n",
    "#     tokens = ROMANCE_en_tokenizer.tokenize(selection)\n",
    "#     selected_token = ROMANCE_en_tokenizer.convert_tokens_to_ids(tokens)[0]\n",
    "#     # list(zip(ROMANCE_en_tokenizer.convert_tokens_to_ids(tokens), tokens))\n",
    "\n",
    "#     original_postprocess = False\n",
    "#     top50 = translate(ROMANCE_en_tokenizer, Taylor reportedly consumed copious amounts of raw fruit and iced milk while attending holiday celebrations during a fundraising event at the Washington Monument.ROMANCE_en, \">>en<<\" + machine_translation, 10)\n",
    "#     for element in top50[0:1]:\n",
    "#         results.append(score_prefix(machine_translation, element))\n",
    "        \n",
    "# all_sorted = sorted(((score, result) for score, result in results), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/39100652/python-chunking-others-than-noun-phrases-e-g-prepositional-using-spacy-etc\n",
    "def get_pps(doc):\n",
    "    pps = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADP':\n",
    "            pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            pps.append(pp)\n",
    "        if token.dep_ == 'prep':\n",
    "            off_limits.append(' '.join([tok.orth_ for tok in token.subtree]))\n",
    "    return pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_clause(doc):\n",
    "    clauses = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'advcl' or token.dep_ == 'npadvmod' or token.dep_ == 'advmod':\n",
    "            clause = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            clauses.append(clause)\n",
    "    return clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I ', 'My birthday', 'Because my birthday is tomorrow', 'Tomorrow ']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I'm excited because my birthday is tomorrow\"\n",
    "phrases = []\n",
    "doc = nlp(sentence)\n",
    "\n",
    "#get prepositional phrases and blacklist OPs \n",
    "off_limits = []\n",
    "for pphrase in get_pps(doc):\n",
    "    #messy way to capitalize the first word without lowercasing the others\n",
    "    capitalized = pphrase.split(' ')[0].capitalize() + ' ' + ' '.join(pphrase.split(' ')[1:])\n",
    "    phrases.append(capitalized)\n",
    "\n",
    "#get noun chunks that aren't OPs\n",
    "for chunk in doc.noun_chunks:\n",
    "    valid = True\n",
    "    for phr in off_limits:\n",
    "        if chunk.text in phr:\n",
    "            valid = False\n",
    "    if valid:\n",
    "        capitalized = chunk.text.split(' ')[0].capitalize() + ' ' + ' '.join(chunk.text.split(' ')[1:])\n",
    "        phrases.append(capitalized)\n",
    "\n",
    "#get adverbial modifiers and clauses\n",
    "for clause in get_adv_clause(doc):\n",
    "    capitalized = clause.split(' ')[0].capitalize() + ' ' + ' '.join(clause.split(' ')[1:])\n",
    "    phrases.append(capitalized)\n",
    "\n",
    "print(phrases)\n",
    "\n",
    "original_postprocess = True;\n",
    "english = \">>es<<\" + sentence\n",
    "engbatch = en_ROMANCE_tokenizer.prepare_translation_batch([english]).to(device)\n",
    "eng_to_spanish = en_ROMANCE.generate(**engbatch).to(device)\n",
    "machine_translation = en_ROMANCE_tokenizer.decode(eng_to_spanish[0]).replace(\"<pad> \", '')\n",
    "\n",
    "results = []\n",
    "for selection in set(phrases):\n",
    "    ROMANCE_en_tokenizer.current_spm = ROMANCE_en_tokenizer.spm_target\n",
    "    tokens = ROMANCE_en_tokenizer.tokenize(selection)\n",
    "    selected_tokens = ROMANCE_en_tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # list(zip(ROMANCE_en_tokenizer.convert_tokens_to_ids(tokens), tokens))\n",
    "\n",
    "    original_postprocess = False\n",
    "    top50 = translate(ROMANCE_en_tokenizer, ROMANCE_en, \">>en<<\" + machine_translation, 50)\n",
    "    for element in top50[0:1]:\n",
    "        results.append(score_prefix(machine_translation, element))\n",
    "        \n",
    "all_sorted = sorted(((score, result) for score, result in results), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >sentence</th>        <th class=\"col_heading level0 col1\" >probability</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63row0_col0\" class=\"data row0 col0\" >I'm excited because my birthday is tomorrow.</td>\n",
       "                        <td id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63row0_col1\" class=\"data row0 col1\" >-0.293000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63row1_col0\" class=\"data row1 col0\" >My birthday's tomorrow. I'm so excited.</td>\n",
       "                        <td id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63row1_col1\" class=\"data row1 col1\" >-0.954000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63row2_col0\" class=\"data row2 col0\" >Because my birthday is tomorrow, I'm excited.</td>\n",
       "                        <td id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63row2_col1\" class=\"data row2 col1\" >-1.009000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63row3_col0\" class=\"data row3 col0\" >Tomorrow's the day that I'm excited to have my birthday. I'm so excited that I'm going to have to have my birthday tomorrow.</td>\n",
       "                        <td id=\"T_4a44ef1c_c617_11ea_aeaa_b42e993e0d63row3_col1\" class=\"data row3 col1\" >-1.321000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fee0ae9ffd0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'sentence': [pair[1] for pair in all_sorted],\n",
    "              'probability': [pair[0] for pair in all_sorted]}).style.hide_index()\n",
    "# df = df.style.set_properties(**{'text-align': 'left'})\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
